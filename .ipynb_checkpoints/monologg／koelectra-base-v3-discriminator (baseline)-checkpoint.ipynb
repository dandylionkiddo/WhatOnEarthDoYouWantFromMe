{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18978,
     "status": "ok",
     "timestamp": 1746460244882,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "4d-qFWi9y6p2",
    "outputId": "811fd0df-496f-4f23-ffe6-2c6103e2a96b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qj0mQgvhzH-g"
   },
   "source": [
    "1. 나눔 폰트 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4529,
     "status": "ok",
     "timestamp": 1746461162542,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "MpWT7IdJzNgC",
    "outputId": "13ed72b5-ba7a-461f-c4a4-c344edf1b5fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "fonts-nanum is already the newest version (20200506-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
      "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
      "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
      "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
      "/usr/share/fonts/truetype/nanum: caching, new cache contents: 12 fonts, 0 dirs\n",
      "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
      "/root/.local/share/fonts: skipping, no such directory\n",
      "/root/.fonts: skipping, no such directory\n",
      "/usr/share/fonts/truetype: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
      "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
      "/var/cache/fontconfig: cleaning cache directory\n",
      "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
      "/root/.fontconfig: not cleaning non-existent cache directory\n",
      "fc-cache: succeeded\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqDV3jvEzR94"
   },
   "source": [
    "2. Colab 런타임 다시 시작\n",
    "상단탭에서 [런타임] > [런타임 다시 시작] 클릭\n",
    "3. matplotlib 폰트 변경\n",
    "```\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "```\n",
    "출처: https://developnote.tistory.com/165 [범범범즈의 개발 노트:티스토리]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhgfHYcIzC5D"
   },
   "source": [
    "코랩에서 상대 경로를 사용하여 파일을 불러오는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746461251716,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "StioiKLwzEmy",
    "outputId": "e74b2ce4-2321-496b-be18-a1c6af233485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "# 1. 먼저 현재 작업 디렉토리를 확인합니다:\n",
    "import os\n",
    "print(os.getcwd())\n",
    "# 2. 필요한 경우 작업 디렉토리를 변경합니다:\n",
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/아이펠/DLThon/')#←('/content')  # 코랩의 기본 디렉토리로 변경\n",
    "# 3. 그 다음 상대 경로를 사용하여 파일을 불러올 수 있습니다:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cv4TkP0yzLDf"
   },
   "source": [
    "여기까지 하고 시작\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5Bayi0UYr-X"
   },
   "source": [
    "# preprocessing.py 의 기존 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rt5DZxULYebk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 1. 텍스트 정제 함수들\n",
    "\n",
    "def remove_stopwords(tokens, stopword_list):\n",
    "    stopword_set = stopword_list if isinstance(stopword_list, set) else set(stopword_list)\n",
    "    return [token for token in tokens if token not in stopword_set]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_repetitions(text, repeat_limit=2):\n",
    "    # 문자 반복 (예: ㅋㅋㅋㅋ → ㅋㅋ)\n",
    "    text = re.sub(r'(.)\\1{2,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    # 음절 반복 (예: 하하하하 → 하하)\n",
    "    text = re.sub(r'((..))\\1{1,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# 2. 텍스트 정제\n",
    "\n",
    "def tokenize_and_clean_text(text, stopword_list=None, repeat_limit=2):\n",
    "    text = normalize_repetitions(text, repeat_limit=repeat_limit)\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "    if stopword_list:\n",
    "        tokens = remove_stopwords(tokens, stopword_list)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# 3. 한줄 단위 전처리\n",
    "def preprocess_conversation_lines(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    use_silence=False,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    lines = text.strip().split('\\n')\n",
    "    results = []\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            processed = [\"[SILENCE]\"] if use_silence else []\n",
    "        else:\n",
    "            processed = tokenize_and_clean_text(line, stopword_list=stopwords, repeat_limit=repeat_limit)\n",
    "            if not processed:\n",
    "                processed = [\"[SILENCE]\"] if use_silence else []\n",
    "            else:\n",
    "                processed = [speaker_token] + processed\n",
    "\n",
    "        if processed:\n",
    "            results.append(\" \".join(processed).strip())\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# 4. 여러줄을 한 줄로 flatten 함수\n",
    "def flatten_utterances(utterance_tokens_list, sep_token=\" \"):\n",
    "    return sep_token.join(utterance_tokens_list).strip()\n",
    "\n",
    "\n",
    "# 5.  전체 전처리 파이프라인\n",
    "def preprocess(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    use_silence=True,\n",
    "    sep_token=\" \",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    \"\"\"\n",
    "    전체 전처리 통합 함수\n",
    "    \"\"\"\n",
    "    utterance_tokens = preprocess_conversation_lines(\n",
    "        text,\n",
    "        stopwords=stopwords,\n",
    "        use_silence=use_silence,\n",
    "        speaker_token=speaker_token,\n",
    "        repeat_limit=repeat_limit\n",
    "    )\n",
    "    return flatten_utterances(utterance_tokens, sep_token=sep_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWLU5iVPysQ7"
   },
   "source": [
    "#### 1. 줄 바꿈 표시 X, 불용어 X\n",
    "- 베이스라인\n",
    "- 외부 데이터 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 515084,
     "status": "ok",
     "timestamp": 1746461785146,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "WpbtewEyysQ8",
    "outputId": "2c1b34ce-318e-4de6-9894-6c867aa92741",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "학습 데이터 크기: 3477\n",
      "검증 데이터 크기: 745\n",
      "테스트 데이터 크기: 746\n",
      "학습 데이터의 클래스 분포:\n",
      "class\n",
      "4    786\n",
      "3    707\n",
      "1    681\n",
      "2    679\n",
      "0    624\n",
      "Name: count, dtype: int64\n",
      "모델 'monologg/koelectra-base-v3-discriminator' 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda:0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:11<00:00,  1.52it/s, loss=1.1960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:06<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6995\n",
      "Validation Macro F1: 0.8274\n",
      "모델 저장됨: best_model.pth (F1: 0.8274)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=0.5621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4244\n",
      "Validation Macro F1: 0.8759\n",
      "모델 저장됨: best_model.pth (F1: 0.8759)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=0.3428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3671\n",
      "Validation Macro F1: 0.8888\n",
      "모델 저장됨: best_model.pth (F1: 0.8888)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=0.2573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3416\n",
      "Validation Macro F1: 0.8930\n",
      "모델 저장됨: best_model.pth (F1: 0.8930)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=0.1993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3490\n",
      "Validation Macro F1: 0.8887\n",
      "성능 향상 없음: 1/2\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=0.1842]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3490\n",
      "Validation Macro F1: 0.8887\n",
      "성능 향상 없음: 2/2\n",
      "\n",
      "Early Stopping! 2번의 에폭 동안 성능 향상 없음.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 결과:\n",
      "Test Loss: 0.3371\n",
      "Test Macro F1: 0.8977\n",
      "\n",
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       협박 대화       0.85      0.84      0.85       134\n",
      "       갈취 대화       0.87      0.85      0.86       146\n",
      " 직장 내 괴롭힘 대화       0.96      0.94      0.95       145\n",
      "   기타 괴롭힘 대화       0.82      0.86      0.84       152\n",
      "       일반 대화       0.99      0.99      0.99       169\n",
      "\n",
      "    accuracy                           0.90       746\n",
      "   macro avg       0.90      0.90      0.90       746\n",
      "weighted avg       0.90      0.90      0.90       746\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "테스트 예측 중: 100%|██████████| 16/16 [00:03<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일 생성 완료: submission.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification#, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 설정값\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # 5에서 10으로 변경\n",
    "SCHEDULER_EPOCHS = 5  # 스케줄러용 에폭 (원래 설정 유지)\n",
    "EARLY_STOPPING_PATIENCE = 2  # 연속 2번의 에폭 동안 성능이 향상되지 않으면 학습 중단\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_FILE = \"data/train_preprocessed_1.csv\" # 줄 바꿈 표시 x, 불용어 X\n",
    "TEXT_COL = \"text\"  # 'conversation' 대신 실제 열 이름으로 변경\n",
    "LABEL_COL = \"class\"\n",
    "LABEL_DICT = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_DICT.items()}\n",
    "GPU_NUM = 0\n",
    "TEST_FILE = \"data/test.csv\"\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "print(\"데이터 로드 중...\")\n",
    "df = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# 라벨 매핑\n",
    "df[LABEL_COL] = df[LABEL_COL].map(LABEL_DICT)\n",
    "\n",
    "# 데이터 분할\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[LABEL_COL])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[LABEL_COL])\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_df)}\")\n",
    "print(f\"검증 데이터 크기: {len(val_df)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_df)}\")\n",
    "\n",
    "# 클래스별 데이터 개수 확인\n",
    "print(\"학습 데이터의 클래스 분포:\")\n",
    "print(train_df[LABEL_COL].value_counts())\n",
    "\n",
    "# 2. Dataset 클래스 정의\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. 데이터 로더 생성\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = TextClassificationDataset(\n",
    "        texts=df[TEXT_COL].values,\n",
    "        labels=df[LABEL_COL].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "# 4. 토크나이저 및 모델 로드\n",
    "print(f\"모델 '{MODEL_NAME}' 로드 중...\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "\n",
    "# 5. 데이터 로더 생성\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# 6. 모델 학습 함수\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"학습 중\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\"loss\": f\"{np.mean(losses):.4f}\"})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# 7. 모델 평가 함수\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"평가 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            losses.append(outputs.loss.item())\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "            real_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses), predictions, real_labels\n",
    "\n",
    "# 8. 테스트 세트 예측 함수\n",
    "def predict_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 9. 혼동 행렬 시각화 함수\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    # 한글 폰트 설정\n",
    "    import matplotlib.font_manager as fm\n",
    "\n",
    "    # 시스템에 설치된 폰트 경로 확인\n",
    "    # 리눅스 환경이면 'NanumGothic'이나 다른 한글 폰트 사용\n",
    "    # 윈도우라면 'Malgun Gothic' 등 사용\n",
    "    try:\n",
    "        # 방법 1: 나눔 글꼴 설치 및 사용\n",
    "        !apt-get update -qq\n",
    "        !apt-get install fonts-nanum -qq\n",
    "        plt.rc('font', family='NanumGothic')\n",
    "    except:\n",
    "        try:\n",
    "            # 방법 2: 한글 레이블을 영어로 변환\n",
    "            classes = ['Threat', 'Extortion', 'Workplace Harassment', 'Other Harassment', 'Normal Conversation']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('예측값')\n",
    "    plt.ylabel('실제값')\n",
    "    plt.title('혼동 행렬')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# 10. 학습 메인 함수\n",
    "def train():\n",
    "    # GPU 사용 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 옵티마이저 및 스케줄러 설정\n",
    "    # AdamW 임포트 부분 수정\n",
    "    # from transformers import AdamW - 이 부분이 오류 발생\n",
    "    from torch.optim import AdamW  # 이렇게 수정\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # correct_bias 파라미터 제거#←optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
    "    total_steps = len(train_data_loader) * SCHEDULER_EPOCHS#←total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0  # Early Stopping을 위한 카운터\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        # 학습\n",
    "        train_loss = train_model(model, train_data_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # 검증\n",
    "        val_loss, val_predictions, val_labels = evaluate_model(model, val_data_loader, device)\n",
    "        val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        # 최고 성능 모델 저장 및 Early Stopping 로직\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"모델 저장됨: best_model.pth (F1: {val_f1:.4f})\")\n",
    "            patience_counter = 0  # 성능이 향상되었으므로 카운터 초기화\n",
    "        else:\n",
    "            patience_counter += 1  # 성능이 향상되지 않았으므로 카운터 증가\n",
    "            print(f\"성능 향상 없음: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "            # Early Stopping 조건 확인\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly Stopping! {EARLY_STOPPING_PATIENCE}번의 에폭 동안 성능 향상 없음.\")\n",
    "                break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_predictions, test_labels = evaluate_model(model, test_data_loader, device)\n",
    "    test_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "    print(\"\\n테스트 결과:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "    # 분류 보고서 출력\n",
    "    class_names = [ID_TO_LABEL[i] for i in range(len(LABEL_DICT))]\n",
    "    print(\"\\n분류 보고서:\")\n",
    "    print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 11. 실제 테스트 데이터 예측 및 제출 파일 생성\n",
    "def predict_and_save():\n",
    "    # 테스트 파일 로드\n",
    "    test_data = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    # 텍스트 전처리 적용\n",
    "    from functools import partial\n",
    "\n",
    "    # 전처리 함수 불러오기 (위에서 사용한 함수를 가정)\n",
    "#     from preprocessing import preprocess\n",
    "\n",
    "    # 불용어 리스트 (제공된 코드에서 가져옴)\n",
    "    stopwords = '이 있 하 것 들 그 되 수 이 보 않 없 나 주 아니 등 같 우리 때 년 가 한 지 오 네 야 아 아니 그럼 내가 너'.split()\n",
    "\n",
    "    # 전처리 함수 설정 (train_preprocessed_2.csv와 동일한 설정)\n",
    "    preprocess_fn = partial(\n",
    "        preprocess,\n",
    "        stopwords=None,   # 불용어 리스트  default: None\n",
    "        speaker_token=\"\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "        sep_token=\" \", # 줄 구분 시 토큰\n",
    "        use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "        repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    "    )\n",
    "\n",
    "    # 전처리 적용\n",
    "    test_data['processed_text'] = test_data['text'].apply(preprocess_fn)\n",
    "\n",
    "    # Dataset 생성\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_token_type_ids=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            }\n",
    "\n",
    "    # 테스트 데이터 로더 생성\n",
    "    test_dataset = TestDataset(\n",
    "        texts=test_data['processed_text'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # GPU 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # 예측\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    # 예측 결과 라벨로 변환\n",
    "    label_predictions = [str(p) for p in predictions]#←label_predictions = [ID_TO_LABEL[p] for p in predictions]\n",
    "\n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'idx': test_data['idx'],\n",
    "        'class': label_predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv('data/submission.csv', index=False)\n",
    "    print(f\"제출 파일 생성 완료: submission.csv\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 학습\n",
    "    model = train()\n",
    "\n",
    "    # 테스트 데이터 예측 및 제출 파일 생성\n",
    "    predict_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 488066,
     "status": "ok",
     "timestamp": 1746462427537,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "RDLuGcvSysQ9",
    "outputId": "06a8fdee-450f-48a4-a46a-e2b40199e207",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리 및 로드 중...\n",
      "데이터 전처리 시작...\n",
      "   idx      class                                               text\n",
      "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가 아닙니다 죄송합니다 죽을 거면 혼자 죽...\n",
      "1    1      협박 대화  길동경찰서입니다 9시 40분 마트에 폭발물을 설치할거다 네 똑바로 들어 한번만 더 ...\n",
      "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지 나보다 작은 남자는 첨봤어 그만해 니들 놀리는거 재미없어 지...\n",
      "3    3      갈취 대화  어이 거기 예 너 말이야 너 이리 오라고 무슨 일 너 옷 좋아보인다 얘 돈 좀 있나...\n",
      "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 이 선크림 파는데 한 번 손등에 발...\n",
      "학습 데이터 크기: 3391\n",
      "검증 데이터 크기: 727\n",
      "테스트 데이터 크기: 727\n",
      "학습 데이터의 클래스 분포:\n",
      "class\n",
      "3    707\n",
      "4    700\n",
      "1    681\n",
      "2    679\n",
      "0    624\n",
      "Name: count, dtype: int64\n",
      "모델 'monologg/koelectra-base-v3-discriminator' 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda:0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.48it/s, loss=1.1932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6991\n",
      "Validation Macro F1: 0.8441\n",
      "모델 저장됨: best_model.pth (F1: 0.8441)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.47it/s, loss=0.5478]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4135\n",
      "Validation Macro F1: 0.8888\n",
      "모델 저장됨: best_model.pth (F1: 0.8888)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.49it/s, loss=0.3451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3993\n",
      "Validation Macro F1: 0.8738\n",
      "성능 향상 없음: 1/2\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:10<00:00,  1.49it/s, loss=0.2421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3515\n",
      "Validation Macro F1: 0.8891\n",
      "모델 저장됨: best_model.pth (F1: 0.8891)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.49it/s, loss=0.1882]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3577\n",
      "Validation Macro F1: 0.8862\n",
      "성능 향상 없음: 1/2\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.49it/s, loss=0.1719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3577\n",
      "Validation Macro F1: 0.8862\n",
      "성능 향상 없음: 2/2\n",
      "\n",
      "Early Stopping! 2번의 에폭 동안 성능 향상 없음.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 결과:\n",
      "Test Loss: 0.3366\n",
      "Test Macro F1: 0.9033\n",
      "\n",
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       협박 대화       0.89      0.78      0.83       134\n",
      "       갈취 대화       0.90      0.89      0.90       146\n",
      " 직장 내 괴롭힘 대화       0.94      0.96      0.95       146\n",
      "   기타 괴롭힘 대화       0.81      0.89      0.85       151\n",
      "       일반 대화       0.99      1.00      1.00       150\n",
      "\n",
      "    accuracy                           0.91       727\n",
      "   macro avg       0.91      0.90      0.90       727\n",
      "weighted avg       0.91      0.91      0.90       727\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "테스트 예측 중: 100%|██████████| 16/16 [00:03<00:00,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일 생성 완료: data/submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification#, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "#--------------------------\n",
    "# 데이터 전처리 함수들\n",
    "#--------------------------\n",
    "\n",
    "# 1. 텍스트 정제 함수들\n",
    "def remove_stopwords(tokens, stopword_list):\n",
    "    stopword_set = stopword_list if isinstance(stopword_list, set) else set(stopword_list)\n",
    "    return [token for token in tokens if token not in stopword_set]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_repetitions(text, repeat_limit=2):\n",
    "    # 문자 반복 (예: ㅋㅋㅋㅋ → ㅋㅋ)\n",
    "    text = re.sub(r'(.)\\1{2,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    # 음절 반복 (예: 하하하하 → 하하)\n",
    "    text = re.sub(r'((..))\\\\1{1,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# 2. 텍스트 정제\n",
    "def tokenize_and_clean_text(text, stopword_list=None, repeat_limit=2):\n",
    "    text = normalize_repetitions(text, repeat_limit=repeat_limit)\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "    if stopword_list:\n",
    "        tokens = remove_stopwords(tokens, stopword_list)\n",
    "    return tokens\n",
    "\n",
    "# 3. 한줄 단위 전처리\n",
    "def preprocess_conversation_lines(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    use_silence=False,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    lines = text.strip().split('\\n')\n",
    "    results = []\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            processed = [\"[SILENCE]\"] if use_silence else []\n",
    "        else:\n",
    "            processed = tokenize_and_clean_text(line, stopword_list=stopwords, repeat_limit=repeat_limit)\n",
    "            if not processed:\n",
    "                processed = [\"[SILENCE]\"] if use_silence else []\n",
    "            else:\n",
    "                processed = [speaker_token] + processed if speaker_token else processed\n",
    "\n",
    "        if processed:\n",
    "            results.append(\" \".join(processed).strip())\n",
    "\n",
    "    return results\n",
    "\n",
    "# 4. 여러줄을 한 줄로 flatten 함수\n",
    "def flatten_utterances(utterance_tokens_list, sep_token=\" \"):\n",
    "    return sep_token.join(utterance_tokens_list).strip()\n",
    "\n",
    "# 5. 전체 전처리 파이프라인\n",
    "def preprocess(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    use_silence=True,\n",
    "    sep_token=\" \",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    \"\"\"\n",
    "    전체 전처리 통합 함수\n",
    "    \"\"\"\n",
    "    utterance_tokens = preprocess_conversation_lines(\n",
    "        text,\n",
    "        stopwords=stopwords,\n",
    "        use_silence=use_silence,\n",
    "        speaker_token=speaker_token,\n",
    "        repeat_limit=repeat_limit\n",
    "    )\n",
    "    return flatten_utterances(utterance_tokens, sep_token=sep_token)\n",
    "\n",
    "# 결측치 및 중복 제거 함수\n",
    "def clean_dataframe(df, is_train=True):\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    if is_train:\n",
    "        df = df.dropna(subset=['class'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# CSV 파일 로드 함수\n",
    "def load_csv_files(file_list, is_train=True):\n",
    "    df_list = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if is_train:\n",
    "            if 'conversation' in df.columns:\n",
    "                df = df.rename(columns={'conversation': 'text'})\n",
    "        else:\n",
    "            if 'conversation' in df.columns:\n",
    "                df = df.rename(columns={'conversation': 'text'})\n",
    "            if 'class' not in df.columns:\n",
    "                df['class'] = pd.NA  # test에는 class가 없으므로 NaN 처리\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 데이터프레임 준비 함수\n",
    "def prepare_dataset(file_paths, preprocess_func, is_train=True):\n",
    "    df = load_csv_files(file_paths, is_train=is_train)\n",
    "    df = clean_dataframe(df, is_train=is_train)\n",
    "    df['text'] = df['text'].apply(preprocess_func)\n",
    "    df = clean_dataframe(df, is_train=is_train)\n",
    "    return df\n",
    "\n",
    "#--------------------------\n",
    "# 모델 학습 설정\n",
    "#--------------------------\n",
    "\n",
    "# 설정값\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # 5에서 10으로 변경\n",
    "SCHEDULER_EPOCHS = 5  # 스케줄러용 에폭 (원래 설정 유지)\n",
    "EARLY_STOPPING_PATIENCE = 2  # 연속 2번의 에폭 동안 성능이 향상되지 않으면 학습 중단\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_PATH = \"data/merged_train.csv\"\n",
    "TEXT_COL = \"text\"#\"conversation\"x : 편리성 때문에\n",
    "LABEL_COL = \"class\"\n",
    "LABEL_DICT = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_DICT.items()}\n",
    "GPU_NUM = 0\n",
    "TEST_FILE = \"data/test.csv\"\n",
    "\n",
    "# 1. 데이터 전처리 및 로드\n",
    "print(\"데이터 전처리 및 로드 중...\")\n",
    "\n",
    "# 불용어 리스트 정의\n",
    "stopwords = '이 있 하 것 들 그 되 수 이 보 않 없 나 주 아니 등 같 우리 때 년 가 한 지 오 네 야 아 아니 그럼 내가 너'.split()\n",
    "\n",
    "# 전처리 함수 설정 - 줄 바꿈 표시 X, 불용어 X\n",
    "preprocess_fn = partial(\n",
    "    preprocess,\n",
    "    stopwords=None,   # 불용어 리스트 사용하지 않음\n",
    "    speaker_token=\"\",  # 발화 단위 토큰 사용하지 않음\n",
    "    sep_token=\" \", # 줄 구분 시 토큰\n",
    "    use_silence=False, # [SILENCE] 토큰 사용하지 않음\n",
    "    repeat_limit=2 # 반복 문자 2개까지 허용\n",
    ")\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_files = [TRAIN_PATH]\n",
    "\n",
    "# 데이터 전처리 및 저장\n",
    "print(\"데이터 전처리 시작...\")\n",
    "train_df = prepare_dataset(train_files, preprocess_fn, is_train=True)\n",
    "# train_df.to_csv(\"dataset/train_preprocessed_1.csv\", index=False)\n",
    "# print(f\"✅ 전처리된 데이터 저장 완료: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "\n",
    "# 라벨 매핑\n",
    "train_df[LABEL_COL] = train_df[LABEL_COL].map(LABEL_DICT)\n",
    "\n",
    "# 데이터 분할\n",
    "train_data, temp_data = train_test_split(train_df, test_size=0.3, random_state=42, stratify=train_df[LABEL_COL])\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data[LABEL_COL])\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_data)}\")\n",
    "print(f\"검증 데이터 크기: {len(val_data)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_data)}\")\n",
    "\n",
    "# 클래스별 데이터 개수 확인\n",
    "print(\"학습 데이터의 클래스 분포:\")\n",
    "print(train_data[LABEL_COL].value_counts())\n",
    "\n",
    "# 2. Dataset 클래스 정의\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. 데이터 로더 생성\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = TextClassificationDataset(\n",
    "        texts=df[TEXT_COL].values,\n",
    "        labels=df[LABEL_COL].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "# 4. 토크나이저 및 모델 로드\n",
    "print(f\"모델 '{MODEL_NAME}' 로드 중...\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "\n",
    "# 5. 데이터 로더 생성\n",
    "train_data_loader = create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# 6. 모델 학습 함수\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"학습 중\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\"loss\": f\"{np.mean(losses):.4f}\"})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# 7. 모델 평가 함수\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"평가 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            losses.append(outputs.loss.item())\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "            real_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses), predictions, real_labels\n",
    "\n",
    "# 8. 테스트 세트 예측 함수\n",
    "def predict_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 9. 혼동 행렬 시각화 함수\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    # 한글 폰트 설정\n",
    "    import matplotlib.font_manager as fm\n",
    "\n",
    "    # 시스템에 설치된 폰트 경로 확인\n",
    "    # 리눅스 환경이면 'NanumGothic'이나 다른 한글 폰트 사용\n",
    "    # 윈도우라면 'Malgun Gothic' 등 사용\n",
    "    try:\n",
    "        # 방법 1: 나눔 글꼴 설치 및 사용\n",
    "        !apt-get update -qq\n",
    "        !apt-get install fonts-nanum -qq\n",
    "        plt.rc('font', family='NanumGothic')\n",
    "    except:\n",
    "        try:\n",
    "            # 방법 2: 한글 레이블을 영어로 변환\n",
    "            classes = ['Threat', 'Extortion', 'Workplace Harassment', 'Other Harassment', 'Normal Conversation']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('예측값')\n",
    "    plt.ylabel('실제값')\n",
    "    plt.title('혼동 행렬')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# 10. 학습 메인 함수\n",
    "def train():\n",
    "    # GPU 사용 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 옵티마이저 및 스케줄러 설정\n",
    "    # AdamW 임포트 부분 수정\n",
    "    # from transformers import AdamW - 이 부분이 오류 발생\n",
    "    from torch.optim import AdamW  # 이렇게 수정\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # correct_bias 파라미터 제거\n",
    "    total_steps = len(train_data_loader) * SCHEDULER_EPOCHS  # 원래 에폭(5) 기준으로 설정\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0  # Early Stopping을 위한 카운터\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        # 학습\n",
    "        train_loss = train_model(model, train_data_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # 검증\n",
    "        val_loss, val_predictions, val_labels = evaluate_model(model, val_data_loader, device)\n",
    "        val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        # 최고 성능 모델 저장 및 Early Stopping 로직\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"모델 저장됨: best_model.pth (F1: {val_f1:.4f})\")\n",
    "            patience_counter = 0  # 성능이 향상되었으므로 카운터 초기화\n",
    "        else:\n",
    "            patience_counter += 1  # 성능이 향상되지 않았으므로 카운터 증가\n",
    "            print(f\"성능 향상 없음: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "            # Early Stopping 조건 확인\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly Stopping! {EARLY_STOPPING_PATIENCE}번의 에폭 동안 성능 향상 없음.\")\n",
    "                break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_predictions, test_labels = evaluate_model(model, test_data_loader, device)\n",
    "    test_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "    print(\"\\n테스트 결과:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "    # 분류 보고서 출력\n",
    "    class_names = [ID_TO_LABEL[i] for i in range(len(LABEL_DICT))]\n",
    "    print(\"\\n분류 보고서:\")\n",
    "    print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 11. 실제 테스트 데이터 예측 및 제출 파일 생성\n",
    "def predict_and_save():\n",
    "    # 테스트 파일 로드\n",
    "    test_data = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    # 텍스트 전처리 적용 - 학습 데이터와 동일한 방식으로 전처리\n",
    "    # 줄 바꿈 표시 X, 불용어 X 설정\n",
    "    test_data['processed_text'] = test_data['text'].apply(\n",
    "        lambda x: preprocess(\n",
    "            x,\n",
    "            stopwords=None,\n",
    "            speaker_token=\"\",\n",
    "            sep_token=\" \",\n",
    "            use_silence=False,\n",
    "            repeat_limit=2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Dataset 생성\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_token_type_ids=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            }\n",
    "\n",
    "    # 테스트 데이터 로더 생성\n",
    "    test_dataset = TestDataset(\n",
    "        texts=test_data['processed_text'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # GPU 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # 예측\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    # 예측 결과 라벨로 변환 (숫자로 유지)\n",
    "    label_predictions = [str(p) for p in predictions]\n",
    "\n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'idx': test_data['idx'],\n",
    "        'class': label_predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv('data/submission.csv', index=False)\n",
    "    print(f\"제출 파일 생성 완료: data/submission.csv\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 학습\n",
    "    model = train()\n",
    "\n",
    "    # 테스트 데이터 예측 및 제출 파일 생성\n",
    "    predict_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3LEsM59ysQ-"
   },
   "source": [
    "#### 2. 줄 바꿈 표시 O, 불용어 X\n",
    "- 베이스라인\n",
    "- 외부 데이터 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102122,
     "status": "ok",
     "timestamp": 1746463109808,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "2WYyiYzTysQ-",
    "outputId": "c4faf722-2d9e-448c-f763-a604bae65e84"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "학습 데이터 크기: 3477\n",
      "검증 데이터 크기: 745\n",
      "테스트 데이터 크기: 746\n",
      "학습 데이터의 클래스 분포:\n",
      "class\n",
      "4    786\n",
      "3    707\n",
      "1    681\n",
      "2    679\n",
      "0    624\n",
      "Name: count, dtype: int64\n",
      "모델 'monologg/koelectra-base-v3-discriminator' 로드 중...\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda:0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=1.2676]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2676\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.52it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8634\n",
      "Validation Macro F1: 0.7153\n",
      "모델 저장됨: best_model.pth (F1: 0.7153)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=0.6684]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6684\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.45it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4917\n",
      "Validation Macro F1: 0.8542\n",
      "모델 저장됨: best_model.pth (F1: 0.8542)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=0.4363]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4363\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.45it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4625\n",
      "Validation Macro F1: 0.8548\n",
      "모델 저장됨: best_model.pth (F1: 0.8548)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=0.3270]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3270\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.45it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3852\n",
      "Validation Macro F1: 0.8733\n",
      "모델 저장됨: best_model.pth (F1: 0.8733)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.47it/s, loss=0.2612]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2612\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.43it/s]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3781\n",
      "Validation Macro F1: 0.8760\n",
      "모델 저장됨: best_model.pth (F1: 0.8760)\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=0.2407]\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3781\n",
      "Validation Macro F1: 0.8760\n",
      "성능 향상 없음: 1/2\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.47it/s, loss=0.2411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3781\n",
      "Validation Macro F1: 0.8760\n",
      "성능 향상 없음: 2/2\n",
      "\n",
      "Early Stopping! 2번의 에폭 동안 성능 향상 없음.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 결과:\n",
      "Test Loss: 0.4048\n",
      "Test Macro F1: 0.8659\n",
      "\n",
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       협박 대화       0.82      0.76      0.79       134\n",
      "       갈취 대화       0.85      0.83      0.84       146\n",
      " 직장 내 괴롭힘 대화       0.88      0.94      0.91       145\n",
      "   기타 괴롭힘 대화       0.79      0.80      0.79       152\n",
      "       일반 대화       0.99      1.00      1.00       169\n",
      "\n",
      "    accuracy                           0.87       746\n",
      "   macro avg       0.87      0.87      0.87       746\n",
      "weighted avg       0.87      0.87      0.87       746\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "테스트 예측 중: 100%|██████████| 16/16 [00:03<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일 생성 완료: submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification#, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 설정값\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # 5에서 10으로 변경\n",
    "SCHEDULER_EPOCHS = 5  # 스케줄러용 에폭 (원래 설정 유지)\n",
    "EARLY_STOPPING_PATIENCE = 2  # 연속 2번의 에폭 동안 성능이 향상되지 않으면 학습 중단\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_FILE = \"data/train_preprocessed_2.csv\" # 줄 바꿈 표시 O, 불용어 X\n",
    "TEXT_COL = \"text\"  # 'conversation' 대신 실제 열 이름으로 변경\n",
    "LABEL_COL = \"class\"\n",
    "LABEL_DICT = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_DICT.items()}\n",
    "GPU_NUM = 0\n",
    "TEST_FILE = \"data/test.csv\"\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "print(\"데이터 로드 중...\")\n",
    "df = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# 라벨 매핑\n",
    "df[LABEL_COL] = df[LABEL_COL].map(LABEL_DICT)\n",
    "\n",
    "# 데이터 분할\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[LABEL_COL])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[LABEL_COL])\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_df)}\")\n",
    "print(f\"검증 데이터 크기: {len(val_df)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_df)}\")\n",
    "\n",
    "# 클래스별 데이터 개수 확인\n",
    "print(\"학습 데이터의 클래스 분포:\")\n",
    "print(train_df[LABEL_COL].value_counts())\n",
    "\n",
    "# 2. Dataset 클래스 정의\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. 데이터 로더 생성\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = TextClassificationDataset(\n",
    "        texts=df[TEXT_COL].values,\n",
    "        labels=df[LABEL_COL].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "# 4. 토크나이저 및 모델 로드\n",
    "print(f\"모델 '{MODEL_NAME}' 로드 중...\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "\n",
    "# 5. 데이터 로더 생성\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# 6. 모델 학습 함수\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"학습 중\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\"loss\": f\"{np.mean(losses):.4f}\"})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# 7. 모델 평가 함수\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"평가 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            losses.append(outputs.loss.item())\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "            real_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses), predictions, real_labels\n",
    "\n",
    "# 8. 테스트 세트 예측 함수\n",
    "def predict_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 9. 혼동 행렬 시각화 함수\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    # 한글 폰트 설정\n",
    "    import matplotlib.font_manager as fm\n",
    "\n",
    "    # 시스템에 설치된 폰트 경로 확인\n",
    "    # 리눅스 환경이면 'NanumGothic'이나 다른 한글 폰트 사용\n",
    "    # 윈도우라면 'Malgun Gothic' 등 사용\n",
    "    try:\n",
    "        # 방법 1: 나눔 글꼴 설치 및 사용\n",
    "        !apt-get update -qq\n",
    "        !apt-get install fonts-nanum -qq\n",
    "        plt.rc('font', family='NanumGothic')\n",
    "    except:\n",
    "        try:\n",
    "            # 방법 2: 한글 레이블을 영어로 변환\n",
    "            classes = ['Threat', 'Extortion', 'Workplace Harassment', 'Other Harassment', 'Normal Conversation']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('예측값')\n",
    "    plt.ylabel('실제값')\n",
    "    plt.title('혼동 행렬')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# 10. 학습 메인 함수\n",
    "def train():\n",
    "    # GPU 사용 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 옵티마이저 및 스케줄러 설정\n",
    "    # AdamW 임포트 부분 수정\n",
    "    # from transformers import AdamW - 이 부분이 오류 발생\n",
    "    from torch.optim import AdamW  # 이렇게 수정\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # correct_bias 파라미터 제거\n",
    "    total_steps = len(train_data_loader) * SCHEDULER_EPOCHS#←total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0  # Early Stopping을 위한 카운터\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        # 학습\n",
    "        train_loss = train_model(model, train_data_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # 검증\n",
    "        val_loss, val_predictions, val_labels = evaluate_model(model, val_data_loader, device)\n",
    "        val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        # 최고 성능 모델 저장 및 Early Stopping 로직\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"모델 저장됨: best_model.pth (F1: {val_f1:.4f})\")\n",
    "            patience_counter = 0  # 성능이 향상되었으므로 카운터 초기화\n",
    "        else:\n",
    "            patience_counter += 1  # 성능이 향상되지 않았으므로 카운터 증가\n",
    "            print(f\"성능 향상 없음: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "            # Early Stopping 조건 확인\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly Stopping! {EARLY_STOPPING_PATIENCE}번의 에폭 동안 성능 향상 없음.\")\n",
    "                break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_predictions, test_labels = evaluate_model(model, test_data_loader, device)\n",
    "    test_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "    print(\"\\n테스트 결과:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "    # 분류 보고서 출력\n",
    "    class_names = [ID_TO_LABEL[i] for i in range(len(LABEL_DICT))]\n",
    "    print(\"\\n분류 보고서:\")\n",
    "    print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 11. 실제 테스트 데이터 예측 및 제출 파일 생성\n",
    "def predict_and_save():\n",
    "    # 테스트 파일 로드\n",
    "    test_data = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    # 텍스트 전처리 적용\n",
    "    from functools import partial\n",
    "\n",
    "    # 전처리 함수 불러오기 (위에서 사용한 함수를 가정)\n",
    "#     from preprocessing import preprocess\n",
    "\n",
    "    # 불용어 리스트 (제공된 코드에서 가져옴)\n",
    "    stopwords = '이 있 하 것 들 그 되 수 이 보 않 없 나 주 아니 등 같 우리 때 년 가 한 지 오 네 야 아 아니 그럼 내가 너'.split()\n",
    "\n",
    "    # 전처리 함수 설정 (train_preprocessed_2.csv와 동일한 설정)\n",
    "    preprocess_fn = partial(\n",
    "        preprocess,\n",
    "        stopwords=None,   # 불용어 리스트  default: None\n",
    "        speaker_token=\"[UTTER]\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "        sep_token=\" \", # 줄 구분 시 토큰\n",
    "        use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "        repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    "    )\n",
    "\n",
    "    # 전처리 적용\n",
    "    test_data['processed_text'] = test_data['text'].apply(preprocess_fn)\n",
    "\n",
    "    # Dataset 생성\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_token_type_ids=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            }\n",
    "\n",
    "    # 테스트 데이터 로더 생성\n",
    "    test_dataset = TestDataset(\n",
    "        texts=test_data['processed_text'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # GPU 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # 예측\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    # 예측 결과 라벨로 변환\n",
    "    label_predictions = [str(p) for p in predictions]#←label_predictions = [ID_TO_LABEL[p] for p in predictions]\n",
    "\n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'idx': test_data['idx'],\n",
    "        'class': label_predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv('data/submission.csv', index=False)\n",
    "    print(f\"제출 파일 생성 완료: submission.csv\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 학습\n",
    "    model = train()\n",
    "\n",
    "    # 테스트 데이터 예측 및 제출 파일 생성\n",
    "    predict_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 335142,
     "status": "ok",
     "timestamp": 1746463545954,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "QH_oFwUaysQ_",
    "outputId": "0edad0a2-ade4-48a2-fb49-3d1e8f68c491"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리 및 로드 중...\n",
      "데이터 전처리 시작...\n",
      "   idx      class                                               text\n",
      "0    0      협박 대화  [UTTER] 지금 너 스스로를 죽여달라고 애원하는 것인가 [UTTER] 아닙니다 ...\n",
      "1    1      협박 대화  [UTTER] 길동경찰서입니다 [UTTER] 9시 40분 마트에 폭발물을 설치할거다...\n",
      "2    2  기타 괴롭힘 대화  [UTTER] 너 되게 귀여운거 알지 나보다 작은 남자는 첨봤어 [UTTER] 그만...\n",
      "3    3      갈취 대화  [UTTER] 어이 거기 [UTTER] 예 [UTTER] 너 말이야 너 이리 오라고...\n",
      "4    4      갈취 대화  [UTTER] 저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 이 선크림 파는데 한...\n",
      "학습 데이터 크기: 3391\n",
      "검증 데이터 크기: 727\n",
      "테스트 데이터 크기: 727\n",
      "학습 데이터의 클래스 분포:\n",
      "class\n",
      "3    707\n",
      "4    700\n",
      "1    681\n",
      "2    679\n",
      "0    624\n",
      "Name: count, dtype: int64\n",
      "모델 'monologg/koelectra-base-v3-discriminator' 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda:0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:13<00:00,  1.45it/s, loss=1.2160]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7991\n",
      "Validation Macro F1: 0.7769\n",
      "모델 저장됨: best_model.pth (F1: 0.7769)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.45it/s, loss=0.6323]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5044\n",
      "Validation Macro F1: 0.8545\n",
      "모델 저장됨: best_model.pth (F1: 0.8545)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.46it/s, loss=0.4259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4546\n",
      "Validation Macro F1: 0.8474\n",
      "성능 향상 없음: 1/2\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.46it/s, loss=0.3283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4422\n",
      "Validation Macro F1: 0.8508\n",
      "성능 향상 없음: 2/2\n",
      "\n",
      "Early Stopping! 2번의 에폭 동안 성능 향상 없음.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 결과:\n",
      "Test Loss: 0.5162\n",
      "Test Macro F1: 0.8456\n",
      "\n",
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       협박 대화       0.87      0.67      0.76       134\n",
      "       갈취 대화       0.79      0.88      0.83       146\n",
      " 직장 내 괴롭힘 대화       0.83      0.95      0.89       146\n",
      "   기타 괴롭힘 대화       0.77      0.74      0.75       151\n",
      "       일반 대화       1.00      1.00      1.00       150\n",
      "\n",
      "    accuracy                           0.85       727\n",
      "   macro avg       0.85      0.85      0.85       727\n",
      "weighted avg       0.85      0.85      0.85       727\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "테스트 예측 중: 100%|██████████| 16/16 [00:03<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일 생성 완료: data/submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification#, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "#--------------------------\n",
    "# 데이터 전처리 함수들\n",
    "#--------------------------\n",
    "\n",
    "# 1. 텍스트 정제 함수들\n",
    "def remove_stopwords(tokens, stopword_list):\n",
    "    stopword_set = stopword_list if isinstance(stopword_list, set) else set(stopword_list)\n",
    "    return [token for token in tokens if token not in stopword_set]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_repetitions(text, repeat_limit=2):\n",
    "    # 문자 반복 (예: ㅋㅋㅋㅋ → ㅋㅋ)\n",
    "    text = re.sub(r'(.)\\1{2,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    # 음절 반복 (예: 하하하하 → 하하)\n",
    "    text = re.sub(r'((..))\\\\1{1,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# 2. 텍스트 정제\n",
    "def tokenize_and_clean_text(text, stopword_list=None, repeat_limit=2):\n",
    "    text = normalize_repetitions(text, repeat_limit=repeat_limit)\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "    if stopword_list:\n",
    "        tokens = remove_stopwords(tokens, stopword_list)\n",
    "    return tokens\n",
    "\n",
    "# 3. 한줄 단위 전처리\n",
    "def preprocess_conversation_lines(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    use_silence=False,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    lines = text.strip().split('\\n')\n",
    "    results = []\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            processed = [\"[SILENCE]\"] if use_silence else []\n",
    "        else:\n",
    "            processed = tokenize_and_clean_text(line, stopword_list=stopwords, repeat_limit=repeat_limit)\n",
    "            if not processed:\n",
    "                processed = [\"[SILENCE]\"] if use_silence else []\n",
    "            else:\n",
    "                processed = [speaker_token] + processed if speaker_token else processed\n",
    "\n",
    "        if processed:\n",
    "            results.append(\" \".join(processed).strip())\n",
    "\n",
    "    return results\n",
    "\n",
    "# 4. 여러줄을 한 줄로 flatten 함수\n",
    "def flatten_utterances(utterance_tokens_list, sep_token=\" \"):\n",
    "    return sep_token.join(utterance_tokens_list).strip()\n",
    "\n",
    "# 5. 전체 전처리 파이프라인\n",
    "def preprocess(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    use_silence=True,\n",
    "    sep_token=\" \",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    \"\"\"\n",
    "    전체 전처리 통합 함수\n",
    "    \"\"\"\n",
    "    utterance_tokens = preprocess_conversation_lines(\n",
    "        text,\n",
    "        stopwords=stopwords,\n",
    "        use_silence=use_silence,\n",
    "        speaker_token=speaker_token,\n",
    "        repeat_limit=repeat_limit\n",
    "    )\n",
    "    return flatten_utterances(utterance_tokens, sep_token=sep_token)\n",
    "\n",
    "# 결측치 및 중복 제거 함수\n",
    "def clean_dataframe(df, is_train=True):\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    if is_train:\n",
    "        df = df.dropna(subset=['class'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# CSV 파일 로드 함수\n",
    "def load_csv_files(file_list, is_train=True):\n",
    "    df_list = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if is_train:\n",
    "            if 'conversation' in df.columns:\n",
    "                df = df.rename(columns={'conversation': 'text'})\n",
    "        else:\n",
    "            if 'conversation' in df.columns:\n",
    "                df = df.rename(columns={'conversation': 'text'})\n",
    "            if 'class' not in df.columns:\n",
    "                df['class'] = pd.NA  # test에는 class가 없으므로 NaN 처리\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 데이터프레임 준비 함수\n",
    "def prepare_dataset(file_paths, preprocess_func, is_train=True):\n",
    "    df = load_csv_files(file_paths, is_train=is_train)\n",
    "    df = clean_dataframe(df, is_train=is_train)\n",
    "    df['text'] = df['text'].apply(preprocess_func)\n",
    "    df = clean_dataframe(df, is_train=is_train)\n",
    "    return df\n",
    "\n",
    "#--------------------------\n",
    "# 모델 학습 설정\n",
    "#--------------------------\n",
    "\n",
    "# 설정값\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # 5에서 10으로 변경\n",
    "SCHEDULER_EPOCHS = 5  # 스케줄러용 에폭 (원래 설정 유지)\n",
    "EARLY_STOPPING_PATIENCE = 2  # 연속 2번의 에폭 동안 성능이 향상되지 않으면 학습 중단\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_PATH = \"data/merged_train.csv\"\n",
    "TEXT_COL = \"text\"#\"conversation\"x : 편리성 때문에\n",
    "LABEL_COL = \"class\"\n",
    "LABEL_DICT = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_DICT.items()}\n",
    "GPU_NUM = 0\n",
    "TEST_FILE = \"data/test.csv\"\n",
    "\n",
    "# 1. 데이터 전처리 및 로드\n",
    "print(\"데이터 전처리 및 로드 중...\")\n",
    "\n",
    "# 불용어 리스트 정의\n",
    "stopwords = '이 있 하 것 들 그 되 수 이 보 않 없 나 주 아니 등 같 우리 때 년 가 한 지 오 네 야 아 아니 그럼 내가 너'.split()\n",
    "\n",
    "# 전처리 함수 설정 - 줄 바꿈 표시 X, 불용어 X\n",
    "preprocess_fn = partial(\n",
    "    preprocess,\n",
    "    stopwords=None,   # 불용어 리스트  default: None\n",
    "    speaker_token=\"[UTTER]\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "    sep_token=\" \", # 줄 구분 시 토큰\n",
    "    use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "    repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    ")\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_files = [TRAIN_PATH]\n",
    "\n",
    "# 데이터 전처리 및 저장\n",
    "print(\"데이터 전처리 시작...\")\n",
    "train_df = prepare_dataset(train_files, preprocess_fn, is_train=True)\n",
    "# train_df.to_csv(\"dataset/train_preprocessed_1.csv\", index=False)\n",
    "# print(f\"✅ 전처리된 데이터 저장 완료: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "\n",
    "# 라벨 매핑\n",
    "train_df[LABEL_COL] = train_df[LABEL_COL].map(LABEL_DICT)\n",
    "\n",
    "# 데이터 분할\n",
    "train_data, temp_data = train_test_split(train_df, test_size=0.3, random_state=42, stratify=train_df[LABEL_COL])\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data[LABEL_COL])\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_data)}\")\n",
    "print(f\"검증 데이터 크기: {len(val_data)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_data)}\")\n",
    "\n",
    "# 클래스별 데이터 개수 확인\n",
    "print(\"학습 데이터의 클래스 분포:\")\n",
    "print(train_data[LABEL_COL].value_counts())\n",
    "\n",
    "# 2. Dataset 클래스 정의\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. 데이터 로더 생성\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = TextClassificationDataset(\n",
    "        texts=df[TEXT_COL].values,\n",
    "        labels=df[LABEL_COL].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "# 4. 토크나이저 및 모델 로드\n",
    "print(f\"모델 '{MODEL_NAME}' 로드 중...\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "\n",
    "# 5. 데이터 로더 생성\n",
    "train_data_loader = create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# 6. 모델 학습 함수\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"학습 중\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\"loss\": f\"{np.mean(losses):.4f}\"})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# 7. 모델 평가 함수\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"평가 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            losses.append(outputs.loss.item())\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "            real_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses), predictions, real_labels\n",
    "\n",
    "# 8. 테스트 세트 예측 함수\n",
    "def predict_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 9. 혼동 행렬 시각화 함수\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    # 한글 폰트 설정\n",
    "    import matplotlib.font_manager as fm\n",
    "\n",
    "    # 시스템에 설치된 폰트 경로 확인\n",
    "    # 리눅스 환경이면 'NanumGothic'이나 다른 한글 폰트 사용\n",
    "    # 윈도우라면 'Malgun Gothic' 등 사용\n",
    "    try:\n",
    "        # 방법 1: 나눔 글꼴 설치 및 사용\n",
    "        !apt-get update -qq\n",
    "        !apt-get install fonts-nanum -qq\n",
    "        plt.rc('font', family='NanumGothic')\n",
    "    except:\n",
    "        try:\n",
    "            # 방법 2: 한글 레이블을 영어로 변환\n",
    "            classes = ['Threat', 'Extortion', 'Workplace Harassment', 'Other Harassment', 'Normal Conversation']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('예측값')\n",
    "    plt.ylabel('실제값')\n",
    "    plt.title('혼동 행렬')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# 10. 학습 메인 함수\n",
    "def train():\n",
    "    # GPU 사용 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 옵티마이저 및 스케줄러 설정\n",
    "    # AdamW 임포트 부분 수정\n",
    "    # from transformers import AdamW - 이 부분이 오류 발생\n",
    "    from torch.optim import AdamW  # 이렇게 수정\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # correct_bias 파라미터 제거\n",
    "    total_steps = len(train_data_loader) * SCHEDULER_EPOCHS  # 원래 에폭(5) 기준으로 설정\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0  # Early Stopping을 위한 카운터\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        # 학습\n",
    "        train_loss = train_model(model, train_data_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # 검증\n",
    "        val_loss, val_predictions, val_labels = evaluate_model(model, val_data_loader, device)\n",
    "        val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        # 최고 성능 모델 저장 및 Early Stopping 로직\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"모델 저장됨: best_model.pth (F1: {val_f1:.4f})\")\n",
    "            patience_counter = 0  # 성능이 향상되었으므로 카운터 초기화\n",
    "        else:\n",
    "            patience_counter += 1  # 성능이 향상되지 않았으므로 카운터 증가\n",
    "            print(f\"성능 향상 없음: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "            # Early Stopping 조건 확인\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly Stopping! {EARLY_STOPPING_PATIENCE}번의 에폭 동안 성능 향상 없음.\")\n",
    "                break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_predictions, test_labels = evaluate_model(model, test_data_loader, device)\n",
    "    test_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "    print(\"\\n테스트 결과:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "    # 분류 보고서 출력\n",
    "    class_names = [ID_TO_LABEL[i] for i in range(len(LABEL_DICT))]\n",
    "    print(\"\\n분류 보고서:\")\n",
    "    print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 11. 실제 테스트 데이터 예측 및 제출 파일 생성\n",
    "def predict_and_save():\n",
    "    # 테스트 파일 로드\n",
    "    test_data = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    # 텍스트 전처리 적용 - 학습 데이터와 동일한 방식으로 전처리\n",
    "    # 줄 바꿈 표시 O, 불용어 X 설정\n",
    "    test_data['processed_text'] = test_data['text'].apply(\n",
    "        lambda x: preprocess(\n",
    "            x,\n",
    "            stopwords=None,   # 불용어 리스트  default: None\n",
    "            speaker_token=\"[UTTER]\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "            sep_token=\" \", # 줄 구분 시 토큰\n",
    "            use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "            repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Dataset 생성\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_token_type_ids=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            }\n",
    "\n",
    "    # 테스트 데이터 로더 생성\n",
    "    test_dataset = TestDataset(\n",
    "        texts=test_data['processed_text'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # GPU 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # 예측\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    # 예측 결과 라벨로 변환 (숫자로 유지)\n",
    "    label_predictions = [str(p) for p in predictions]\n",
    "\n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'idx': test_data['idx'],\n",
    "        'class': label_predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv('data/submission.csv', index=False)\n",
    "    print(f\"제출 파일 생성 완료: data/submission.csv\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 학습\n",
    "    model = train()\n",
    "\n",
    "    # 테스트 데이터 예측 및 제출 파일 생성\n",
    "    predict_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6K-u_zftysQ_"
   },
   "source": [
    "#### 3. 줄 바꿈 표시 X, 불용어 O\n",
    "- 베이스라인\n",
    "- 외부 데이터 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575716,
     "status": "ok",
     "timestamp": 1746464179415,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "1oVSXmqDysQ_",
    "outputId": "39e7f65a-cf1b-4ab4-db60-064ab697a914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "학습 데이터 크기: 3477\n",
      "검증 데이터 크기: 745\n",
      "테스트 데이터 크기: 746\n",
      "학습 데이터의 클래스 분포:\n",
      "class\n",
      "4    786\n",
      "3    707\n",
      "1    681\n",
      "2    679\n",
      "0    624\n",
      "Name: count, dtype: int64\n",
      "모델 'monologg/koelectra-base-v3-discriminator' 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda:0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=1.2085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.7012\n",
      "Validation Macro F1: 0.8324\n",
      "모델 저장됨: best_model.pth (F1: 0.8324)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.49it/s, loss=0.5667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4231\n",
      "Validation Macro F1: 0.8760\n",
      "모델 저장됨: best_model.pth (F1: 0.8760)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=0.3472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3781\n",
      "Validation Macro F1: 0.8837\n",
      "모델 저장됨: best_model.pth (F1: 0.8837)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=0.2458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3601\n",
      "Validation Macro F1: 0.8863\n",
      "모델 저장됨: best_model.pth (F1: 0.8863)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=0.1951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3443\n",
      "Validation Macro F1: 0.8933\n",
      "모델 저장됨: best_model.pth (F1: 0.8933)\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.50it/s, loss=0.1795]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3443\n",
      "Validation Macro F1: 0.8933\n",
      "성능 향상 없음: 1/2\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:12<00:00,  1.51it/s, loss=0.1782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3443\n",
      "Validation Macro F1: 0.8933\n",
      "성능 향상 없음: 2/2\n",
      "\n",
      "Early Stopping! 2번의 에폭 동안 성능 향상 없음.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 결과:\n",
      "Test Loss: 0.3642\n",
      "Test Macro F1: 0.8916\n",
      "\n",
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       협박 대화       0.87      0.83      0.85       134\n",
      "       갈취 대화       0.87      0.86      0.86       146\n",
      " 직장 내 괴롭힘 대화       0.96      0.92      0.94       145\n",
      "   기타 괴롭힘 대화       0.78      0.86      0.82       152\n",
      "       일반 대화       0.99      0.99      0.99       169\n",
      "\n",
      "    accuracy                           0.89       746\n",
      "   macro avg       0.89      0.89      0.89       746\n",
      "weighted avg       0.90      0.89      0.89       746\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "테스트 예측 중: 100%|██████████| 16/16 [00:03<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일 생성 완료: submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification#, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 설정값\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # 5에서 10으로 변경\n",
    "SCHEDULER_EPOCHS = 5  # 스케줄러용 에폭 (원래 설정 유지)\n",
    "EARLY_STOPPING_PATIENCE = 2  # 연속 2번의 에폭 동안 성능이 향상되지 않으면 학습 중단\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_FILE = \"data/train_preprocessed_3.csv\" # 줄 바꿈 표시 x, 불용어 O\n",
    "TEXT_COL = \"text\"  # 'conversation' 대신 실제 열 이름으로 변경\n",
    "LABEL_COL = \"class\"\n",
    "LABEL_DICT = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_DICT.items()}\n",
    "GPU_NUM = 0\n",
    "TEST_FILE = \"data/test.csv\"\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "print(\"데이터 로드 중...\")\n",
    "df = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# 라벨 매핑\n",
    "df[LABEL_COL] = df[LABEL_COL].map(LABEL_DICT)\n",
    "\n",
    "# 데이터 분할\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[LABEL_COL])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[LABEL_COL])\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_df)}\")\n",
    "print(f\"검증 데이터 크기: {len(val_df)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_df)}\")\n",
    "\n",
    "# 클래스별 데이터 개수 확인\n",
    "print(\"학습 데이터의 클래스 분포:\")\n",
    "print(train_df[LABEL_COL].value_counts())\n",
    "\n",
    "# 2. Dataset 클래스 정의\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. 데이터 로더 생성\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = TextClassificationDataset(\n",
    "        texts=df[TEXT_COL].values,\n",
    "        labels=df[LABEL_COL].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "# 4. 토크나이저 및 모델 로드\n",
    "print(f\"모델 '{MODEL_NAME}' 로드 중...\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "\n",
    "# 5. 데이터 로더 생성\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# 6. 모델 학습 함수\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"학습 중\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\"loss\": f\"{np.mean(losses):.4f}\"})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# 7. 모델 평가 함수\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"평가 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            losses.append(outputs.loss.item())\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "            real_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses), predictions, real_labels\n",
    "\n",
    "# 8. 테스트 세트 예측 함수\n",
    "def predict_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 9. 혼동 행렬 시각화 함수\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    # 한글 폰트 설정\n",
    "    import matplotlib.font_manager as fm\n",
    "\n",
    "    # 시스템에 설치된 폰트 경로 확인\n",
    "    # 리눅스 환경이면 'NanumGothic'이나 다른 한글 폰트 사용\n",
    "    # 윈도우라면 'Malgun Gothic' 등 사용\n",
    "    try:\n",
    "        # 방법 1: 나눔 글꼴 설치 및 사용\n",
    "        !apt-get update -qq\n",
    "        !apt-get install fonts-nanum -qq\n",
    "        plt.rc('font', family='NanumGothic')\n",
    "    except:\n",
    "        try:\n",
    "            # 방법 2: 한글 레이블을 영어로 변환\n",
    "            classes = ['Threat', 'Extortion', 'Workplace Harassment', 'Other Harassment', 'Normal Conversation']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('예측값')\n",
    "    plt.ylabel('실제값')\n",
    "    plt.title('혼동 행렬')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# 10. 학습 메인 함수\n",
    "def train():\n",
    "    # GPU 사용 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 옵티마이저 및 스케줄러 설정\n",
    "    # AdamW 임포트 부분 수정\n",
    "    # from transformers import AdamW - 이 부분이 오류 발생\n",
    "    from torch.optim import AdamW  # 이렇게 수정\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # correct_bias 파라미터 제거\n",
    "    total_steps = len(train_data_loader) * SCHEDULER_EPOCHS#←total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0  # Early Stopping을 위한 카운터\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        # 학습\n",
    "        train_loss = train_model(model, train_data_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # 검증\n",
    "        val_loss, val_predictions, val_labels = evaluate_model(model, val_data_loader, device)\n",
    "        val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        # 최고 성능 모델 저장 및 Early Stopping 로직\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"모델 저장됨: best_model.pth (F1: {val_f1:.4f})\")\n",
    "            patience_counter = 0  # 성능이 향상되었으므로 카운터 초기화\n",
    "        else:\n",
    "            patience_counter += 1  # 성능이 향상되지 않았으므로 카운터 증가\n",
    "            print(f\"성능 향상 없음: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "            # Early Stopping 조건 확인\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly Stopping! {EARLY_STOPPING_PATIENCE}번의 에폭 동안 성능 향상 없음.\")\n",
    "                break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_predictions, test_labels = evaluate_model(model, test_data_loader, device)\n",
    "    test_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "    print(\"\\n테스트 결과:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "    # 분류 보고서 출력\n",
    "    class_names = [ID_TO_LABEL[i] for i in range(len(LABEL_DICT))]\n",
    "    print(\"\\n분류 보고서:\")\n",
    "    print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 11. 실제 테스트 데이터 예측 및 제출 파일 생성\n",
    "def predict_and_save():\n",
    "    # 테스트 파일 로드\n",
    "    test_data = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    # 텍스트 전처리 적용\n",
    "    from functools import partial\n",
    "\n",
    "    # 전처리 함수 불러오기 (위에서 사용한 함수를 가정)\n",
    "#     from preprocessing import preprocess\n",
    "\n",
    "    # 불용어 리스트 (제공된 코드에서 가져옴)\n",
    "    stopwords = '이 있 하 것 들 그 되 수 이 보 않 없 나 주 아니 등 같 우리 때 년 가 한 지 오 네 야 아 아니 그럼 내가 너'.split()\n",
    "\n",
    "    # 전처리 함수 설정 (train_preprocessed_3.csv와 동일한 설정)\n",
    "    preprocess_fn = partial(\n",
    "        preprocess,\n",
    "        stopwords=stopwords,   # 불용어 리스트  default: None\n",
    "        speaker_token=\"\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "        sep_token=\" \", # 줄 구분 시 토큰\n",
    "        use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "        repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    "    )\n",
    "\n",
    "    # 전처리 적용\n",
    "    test_data['processed_text'] = test_data['text'].apply(preprocess_fn)\n",
    "\n",
    "    # Dataset 생성\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_token_type_ids=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            }\n",
    "\n",
    "    # 테스트 데이터 로더 생성\n",
    "    test_dataset = TestDataset(\n",
    "        texts=test_data['processed_text'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # GPU 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # 예측\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    # 예측 결과 라벨로 변환\n",
    "    label_predictions = [str(p) for p in predictions]#←label_predictions = [ID_TO_LABEL[p] for p in predictions]\n",
    "\n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'idx': test_data['idx'],\n",
    "        'class': label_predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv('data/submission.csv', index=False)\n",
    "    print(f\"제출 파일 생성 완료: submission.csv\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 학습\n",
    "    model = train()\n",
    "\n",
    "    # 테스트 데이터 예측 및 제출 파일 생성\n",
    "    predict_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 491126,
     "status": "ok",
     "timestamp": 1746464707245,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "DAyXIVkUysRA",
    "outputId": "44f4a3dc-9a77-448b-adaa-4c174c0afbde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리 및 로드 중...\n",
      "데이터 전처리 시작...\n",
      "   idx      class                                               text\n",
      "0    0      협박 대화  지금 스스로를 죽여달라고 애원하는 것인가 아닙니다 죄송합니다 죽을 거면 혼자 죽지 ...\n",
      "1    1      협박 대화  길동경찰서입니다 9시 40분 마트에 폭발물을 설치할거다 똑바로 들어 한번만 더 얘기...\n",
      "2    2  기타 괴롭힘 대화  되게 귀여운거 알지 나보다 작은 남자는 첨봤어 그만해 니들 놀리는거 재미없어 지영아...\n",
      "3    3      갈취 대화  어이 거기 예 말이야 이리 오라고 무슨 일 옷 좋아보인다 얘 돈 좀 있나봐 아니에요...\n",
      "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 선크림 파는데 번 손등에 발라보실래...\n",
      "학습 데이터 크기: 3391\n",
      "검증 데이터 크기: 727\n",
      "테스트 데이터 크기: 727\n",
      "학습 데이터의 클래스 분포:\n",
      "class\n",
      "3    707\n",
      "4    700\n",
      "1    681\n",
      "2    679\n",
      "0    624\n",
      "Name: count, dtype: int64\n",
      "모델 'monologg/koelectra-base-v3-discriminator' 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda:0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.48it/s, loss=1.2018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6913\n",
      "Validation Macro F1: 0.8442\n",
      "모델 저장됨: best_model.pth (F1: 0.8442)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.48it/s, loss=0.5367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4533\n",
      "Validation Macro F1: 0.8593\n",
      "모델 저장됨: best_model.pth (F1: 0.8593)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.49it/s, loss=0.3299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3870\n",
      "Validation Macro F1: 0.8806\n",
      "모델 저장됨: best_model.pth (F1: 0.8806)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.49it/s, loss=0.2374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3592\n",
      "Validation Macro F1: 0.8896\n",
      "모델 저장됨: best_model.pth (F1: 0.8896)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.49it/s, loss=0.1864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3572\n",
      "Validation Macro F1: 0.8891\n",
      "성능 향상 없음: 1/2\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:11<00:00,  1.49it/s, loss=0.1693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3572\n",
      "Validation Macro F1: 0.8891\n",
      "성능 향상 없음: 2/2\n",
      "\n",
      "Early Stopping! 2번의 에폭 동안 성능 향상 없음.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 결과:\n",
      "Test Loss: 0.3453\n",
      "Test Macro F1: 0.8978\n",
      "\n",
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       협박 대화       0.87      0.83      0.85       134\n",
      "       갈취 대화       0.90      0.88      0.89       146\n",
      " 직장 내 괴롭힘 대화       0.95      0.90      0.93       146\n",
      "   기타 괴롭힘 대화       0.80      0.87      0.83       151\n",
      "       일반 대화       0.99      1.00      1.00       150\n",
      "\n",
      "    accuracy                           0.90       727\n",
      "   macro avg       0.90      0.90      0.90       727\n",
      "weighted avg       0.90      0.90      0.90       727\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "테스트 예측 중: 100%|██████████| 16/16 [00:03<00:00,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일 생성 완료: data/submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification#, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "#--------------------------\n",
    "# 데이터 전처리 함수들\n",
    "#--------------------------\n",
    "\n",
    "# 1. 텍스트 정제 함수들\n",
    "def remove_stopwords(tokens, stopword_list):\n",
    "    stopword_set = stopword_list if isinstance(stopword_list, set) else set(stopword_list)\n",
    "    return [token for token in tokens if token not in stopword_set]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_repetitions(text, repeat_limit=2):\n",
    "    # 문자 반복 (예: ㅋㅋㅋㅋ → ㅋㅋ)\n",
    "    text = re.sub(r'(.)\\1{2,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    # 음절 반복 (예: 하하하하 → 하하)\n",
    "    text = re.sub(r'((..))\\\\1{1,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# 2. 텍스트 정제\n",
    "def tokenize_and_clean_text(text, stopword_list=None, repeat_limit=2):\n",
    "    text = normalize_repetitions(text, repeat_limit=repeat_limit)\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "    if stopword_list:\n",
    "        tokens = remove_stopwords(tokens, stopword_list)\n",
    "    return tokens\n",
    "\n",
    "# 3. 한줄 단위 전처리\n",
    "def preprocess_conversation_lines(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    use_silence=False,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    lines = text.strip().split('\\n')\n",
    "    results = []\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            processed = [\"[SILENCE]\"] if use_silence else []\n",
    "        else:\n",
    "            processed = tokenize_and_clean_text(line, stopword_list=stopwords, repeat_limit=repeat_limit)\n",
    "            if not processed:\n",
    "                processed = [\"[SILENCE]\"] if use_silence else []\n",
    "            else:\n",
    "                processed = [speaker_token] + processed if speaker_token else processed\n",
    "\n",
    "        if processed:\n",
    "            results.append(\" \".join(processed).strip())\n",
    "\n",
    "    return results\n",
    "\n",
    "# 4. 여러줄을 한 줄로 flatten 함수\n",
    "def flatten_utterances(utterance_tokens_list, sep_token=\" \"):\n",
    "    return sep_token.join(utterance_tokens_list).strip()\n",
    "\n",
    "# 5. 전체 전처리 파이프라인\n",
    "def preprocess(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    use_silence=True,\n",
    "    sep_token=\" \",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    \"\"\"\n",
    "    전체 전처리 통합 함수\n",
    "    \"\"\"\n",
    "    utterance_tokens = preprocess_conversation_lines(\n",
    "        text,\n",
    "        stopwords=stopwords,\n",
    "        use_silence=use_silence,\n",
    "        speaker_token=speaker_token,\n",
    "        repeat_limit=repeat_limit\n",
    "    )\n",
    "    return flatten_utterances(utterance_tokens, sep_token=sep_token)\n",
    "\n",
    "# 결측치 및 중복 제거 함수\n",
    "def clean_dataframe(df, is_train=True):\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    if is_train:\n",
    "        df = df.dropna(subset=['class'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# CSV 파일 로드 함수\n",
    "def load_csv_files(file_list, is_train=True):\n",
    "    df_list = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if is_train:\n",
    "            if 'conversation' in df.columns:\n",
    "                df = df.rename(columns={'conversation': 'text'})\n",
    "        else:\n",
    "            if 'conversation' in df.columns:\n",
    "                df = df.rename(columns={'conversation': 'text'})\n",
    "            if 'class' not in df.columns:\n",
    "                df['class'] = pd.NA  # test에는 class가 없으므로 NaN 처리\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 데이터프레임 준비 함수\n",
    "def prepare_dataset(file_paths, preprocess_func, is_train=True):\n",
    "    df = load_csv_files(file_paths, is_train=is_train)\n",
    "    df = clean_dataframe(df, is_train=is_train)\n",
    "    df['text'] = df['text'].apply(preprocess_func)\n",
    "    df = clean_dataframe(df, is_train=is_train)\n",
    "    return df\n",
    "\n",
    "#--------------------------\n",
    "# 모델 학습 설정\n",
    "#--------------------------\n",
    "\n",
    "# 설정값\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # 5에서 10으로 변경\n",
    "SCHEDULER_EPOCHS = 5  # 스케줄러용 에폭 (원래 설정 유지)\n",
    "EARLY_STOPPING_PATIENCE = 2  # 연속 2번의 에폭 동안 성능이 향상되지 않으면 학습 중단\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_PATH = \"data/merged_train.csv\"\n",
    "TEXT_COL = \"text\"#\"conversation\"x : 편리성 때문에\n",
    "LABEL_COL = \"class\"\n",
    "LABEL_DICT = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_DICT.items()}\n",
    "GPU_NUM = 0\n",
    "TEST_FILE = \"data/test.csv\"\n",
    "\n",
    "# 1. 데이터 전처리 및 로드\n",
    "print(\"데이터 전처리 및 로드 중...\")\n",
    "\n",
    "# 불용어 리스트 정의\n",
    "stopwords = '이 있 하 것 들 그 되 수 이 보 않 없 나 주 아니 등 같 우리 때 년 가 한 지 오 네 야 아 아니 그럼 내가 너'.split()\n",
    "\n",
    "# 전처리 함수 설정 - 줄 바꿈 표시 X, 불용어 O\n",
    "preprocess_fn = partial(\n",
    "    preprocess,\n",
    "    stopwords=stopwords,   # 불용어 리스트  default: None\n",
    "    speaker_token=\"\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "    sep_token=\" \", # 줄 구분 시 토큰\n",
    "    use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "    repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    ")\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_files = [TRAIN_PATH]\n",
    "\n",
    "# 데이터 전처리 및 저장\n",
    "print(\"데이터 전처리 시작...\")\n",
    "train_df = prepare_dataset(train_files, preprocess_fn, is_train=True)\n",
    "# train_df.to_csv(\"dataset/train_preprocessed_1.csv\", index=False)\n",
    "# print(f\"✅ 전처리된 데이터 저장 완료: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "\n",
    "# 라벨 매핑\n",
    "train_df[LABEL_COL] = train_df[LABEL_COL].map(LABEL_DICT)\n",
    "\n",
    "# 데이터 분할\n",
    "train_data, temp_data = train_test_split(train_df, test_size=0.3, random_state=42, stratify=train_df[LABEL_COL])\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data[LABEL_COL])\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_data)}\")\n",
    "print(f\"검증 데이터 크기: {len(val_data)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_data)}\")\n",
    "\n",
    "# 클래스별 데이터 개수 확인\n",
    "print(\"학습 데이터의 클래스 분포:\")\n",
    "print(train_data[LABEL_COL].value_counts())\n",
    "\n",
    "# 2. Dataset 클래스 정의\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. 데이터 로더 생성\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = TextClassificationDataset(\n",
    "        texts=df[TEXT_COL].values,\n",
    "        labels=df[LABEL_COL].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "# 4. 토크나이저 및 모델 로드\n",
    "print(f\"모델 '{MODEL_NAME}' 로드 중...\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "\n",
    "# 5. 데이터 로더 생성\n",
    "train_data_loader = create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# 6. 모델 학습 함수\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"학습 중\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\"loss\": f\"{np.mean(losses):.4f}\"})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# 7. 모델 평가 함수\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"평가 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            losses.append(outputs.loss.item())\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "            real_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses), predictions, real_labels\n",
    "\n",
    "# 8. 테스트 세트 예측 함수\n",
    "def predict_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 9. 혼동 행렬 시각화 함수\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    # 한글 폰트 설정\n",
    "    import matplotlib.font_manager as fm\n",
    "\n",
    "    # 시스템에 설치된 폰트 경로 확인\n",
    "    # 리눅스 환경이면 'NanumGothic'이나 다른 한글 폰트 사용\n",
    "    # 윈도우라면 'Malgun Gothic' 등 사용\n",
    "    try:\n",
    "        # 방법 1: 나눔 글꼴 설치 및 사용\n",
    "        !apt-get update -qq\n",
    "        !apt-get install fonts-nanum -qq\n",
    "        plt.rc('font', family='NanumGothic')\n",
    "    except:\n",
    "        try:\n",
    "            # 방법 2: 한글 레이블을 영어로 변환\n",
    "            classes = ['Threat', 'Extortion', 'Workplace Harassment', 'Other Harassment', 'Normal Conversation']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('예측값')\n",
    "    plt.ylabel('실제값')\n",
    "    plt.title('혼동 행렬')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# 10. 학습 메인 함수\n",
    "def train():\n",
    "    # GPU 사용 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 옵티마이저 및 스케줄러 설정\n",
    "    # AdamW 임포트 부분 수정\n",
    "    # from transformers import AdamW - 이 부분이 오류 발생\n",
    "    from torch.optim import AdamW  # 이렇게 수정\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # correct_bias 파라미터 제거\n",
    "    total_steps = len(train_data_loader) * SCHEDULER_EPOCHS  # 원래 에폭(5) 기준으로 설정\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0  # Early Stopping을 위한 카운터\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        # 학습\n",
    "        train_loss = train_model(model, train_data_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # 검증\n",
    "        val_loss, val_predictions, val_labels = evaluate_model(model, val_data_loader, device)\n",
    "        val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        # 최고 성능 모델 저장 및 Early Stopping 로직\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"모델 저장됨: best_model.pth (F1: {val_f1:.4f})\")\n",
    "            patience_counter = 0  # 성능이 향상되었으므로 카운터 초기화\n",
    "        else:\n",
    "            patience_counter += 1  # 성능이 향상되지 않았으므로 카운터 증가\n",
    "            print(f\"성능 향상 없음: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "            # Early Stopping 조건 확인\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly Stopping! {EARLY_STOPPING_PATIENCE}번의 에폭 동안 성능 향상 없음.\")\n",
    "                break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_predictions, test_labels = evaluate_model(model, test_data_loader, device)\n",
    "    test_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "    print(\"\\n테스트 결과:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "    # 분류 보고서 출력\n",
    "    class_names = [ID_TO_LABEL[i] for i in range(len(LABEL_DICT))]\n",
    "    print(\"\\n분류 보고서:\")\n",
    "    print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 11. 실제 테스트 데이터 예측 및 제출 파일 생성\n",
    "def predict_and_save():\n",
    "    # 테스트 파일 로드\n",
    "    test_data = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    # 텍스트 전처리 적용 - 학습 데이터와 동일한 방식으로 전처리\n",
    "    # 줄 바꿈 표시 X, 불용어 O 설정\n",
    "    test_data['processed_text'] = test_data['text'].apply(\n",
    "        lambda x: preprocess(\n",
    "            x,\n",
    "            stopwords=stopwords,   # 불용어 리스트  default: None\n",
    "            speaker_token=\"\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "            sep_token=\" \", # 줄 구분 시 토큰\n",
    "            use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "            repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Dataset 생성\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_token_type_ids=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            }\n",
    "\n",
    "    # 테스트 데이터 로더 생성\n",
    "    test_dataset = TestDataset(\n",
    "        texts=test_data['processed_text'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # GPU 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # 예측\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    # 예측 결과 라벨로 변환 (숫자로 유지)\n",
    "    label_predictions = [str(p) for p in predictions]\n",
    "\n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'idx': test_data['idx'],\n",
    "        'class': label_predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv('data/submission.csv', index=False)\n",
    "    print(f\"제출 파일 생성 완료: data/submission.csv\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 학습\n",
    "    model = train()\n",
    "\n",
    "    # 테스트 데이터 예측 및 제출 파일 생성\n",
    "    predict_and_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8JXsr2teysRB"
   },
   "source": [
    "#### 4. 줄 바꿈 표시 O, 불용어 O\n",
    "- 베이스라인\n",
    "- 외부 데이터 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 588769,
     "status": "ok",
     "timestamp": 1746465410936,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "YiLy1MA-ysRB",
    "outputId": "56721d5c-ec0b-4e6c-a772-e503cd8eaaa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "학습 데이터 크기: 3477\n",
      "검증 데이터 크기: 745\n",
      "테스트 데이터 크기: 746\n",
      "학습 데이터의 클래스 분포:\n",
      "class\n",
      "4    786\n",
      "3    707\n",
      "1    681\n",
      "2    679\n",
      "0    624\n",
      "Name: count, dtype: int64\n",
      "모델 'monologg/koelectra-base-v3-discriminator' 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda:0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=1.2844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8893\n",
      "Validation Macro F1: 0.7118\n",
      "모델 저장됨: best_model.pth (F1: 0.7118)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=0.6762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4978\n",
      "Validation Macro F1: 0.8533\n",
      "모델 저장됨: best_model.pth (F1: 0.8533)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=0.4271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4132\n",
      "Validation Macro F1: 0.8664\n",
      "모델 저장됨: best_model.pth (F1: 0.8664)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=0.3117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3794\n",
      "Validation Macro F1: 0.8868\n",
      "모델 저장됨: best_model.pth (F1: 0.8868)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=0.2558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3750\n",
      "Validation Macro F1: 0.8893\n",
      "모델 저장됨: best_model.pth (F1: 0.8893)\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.46it/s, loss=0.2382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3750\n",
      "Validation Macro F1: 0.8893\n",
      "성능 향상 없음: 1/2\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 109/109 [01:14<00:00,  1.47it/s, loss=0.2362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3750\n",
      "Validation Macro F1: 0.8893\n",
      "성능 향상 없음: 2/2\n",
      "\n",
      "Early Stopping! 2번의 에폭 동안 성능 향상 없음.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 24/24 [00:05<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 결과:\n",
      "Test Loss: 0.4146\n",
      "Test Macro F1: 0.8627\n",
      "\n",
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       협박 대화       0.80      0.74      0.77       134\n",
      "       갈취 대화       0.84      0.84      0.84       146\n",
      " 직장 내 괴롭힘 대화       0.88      0.94      0.91       145\n",
      "   기타 괴롭힘 대화       0.80      0.81      0.80       152\n",
      "       일반 대화       0.99      0.99      0.99       169\n",
      "\n",
      "    accuracy                           0.87       746\n",
      "   macro avg       0.86      0.86      0.86       746\n",
      "weighted avg       0.87      0.87      0.87       746\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "테스트 예측 중: 100%|██████████| 16/16 [00:03<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일 생성 완료: submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification#, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 설정값\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # 5에서 10으로 변경\n",
    "SCHEDULER_EPOCHS = 5  # 스케줄러용 에폭 (원래 설정 유지)\n",
    "EARLY_STOPPING_PATIENCE = 2  # 연속 2번의 에폭 동안 성능이 향상되지 않으면 학습 중단\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_FILE = \"data/train_preprocessed_4.csv\" # 줄 바꿈 표시 O, 불용어 O\n",
    "TEXT_COL = \"text\"  # 'conversation' 대신 실제 열 이름으로 변경\n",
    "LABEL_COL = \"class\"\n",
    "LABEL_DICT = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_DICT.items()}\n",
    "GPU_NUM = 0\n",
    "TEST_FILE = \"data/test.csv\"\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "print(\"데이터 로드 중...\")\n",
    "df = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# 라벨 매핑\n",
    "df[LABEL_COL] = df[LABEL_COL].map(LABEL_DICT)\n",
    "\n",
    "# 데이터 분할\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[LABEL_COL])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[LABEL_COL])\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_df)}\")\n",
    "print(f\"검증 데이터 크기: {len(val_df)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_df)}\")\n",
    "\n",
    "# 클래스별 데이터 개수 확인\n",
    "print(\"학습 데이터의 클래스 분포:\")\n",
    "print(train_df[LABEL_COL].value_counts())\n",
    "\n",
    "# 2. Dataset 클래스 정의\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. 데이터 로더 생성\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = TextClassificationDataset(\n",
    "        texts=df[TEXT_COL].values,\n",
    "        labels=df[LABEL_COL].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "# 4. 토크나이저 및 모델 로드\n",
    "print(f\"모델 '{MODEL_NAME}' 로드 중...\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "\n",
    "# 5. 데이터 로더 생성\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# 6. 모델 학습 함수\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"학습 중\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\"loss\": f\"{np.mean(losses):.4f}\"})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# 7. 모델 평가 함수\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"평가 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            losses.append(outputs.loss.item())\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "            real_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses), predictions, real_labels\n",
    "\n",
    "# 8. 테스트 세트 예측 함수\n",
    "def predict_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 9. 혼동 행렬 시각화 함수\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    # 한글 폰트 설정\n",
    "    import matplotlib.font_manager as fm\n",
    "\n",
    "    # 시스템에 설치된 폰트 경로 확인\n",
    "    # 리눅스 환경이면 'NanumGothic'이나 다른 한글 폰트 사용\n",
    "    # 윈도우라면 'Malgun Gothic' 등 사용\n",
    "    try:\n",
    "        # 방법 1: 나눔 글꼴 설치 및 사용\n",
    "        !apt-get update -qq\n",
    "        !apt-get install fonts-nanum -qq\n",
    "        plt.rc('font', family='NanumGothic')\n",
    "    except:\n",
    "        try:\n",
    "            # 방법 2: 한글 레이블을 영어로 변환\n",
    "            classes = ['Threat', 'Extortion', 'Workplace Harassment', 'Other Harassment', 'Normal Conversation']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('예측값')\n",
    "    plt.ylabel('실제값')\n",
    "    plt.title('혼동 행렬')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# 10. 학습 메인 함수\n",
    "def train():\n",
    "    # GPU 사용 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 옵티마이저 및 스케줄러 설정\n",
    "    # AdamW 임포트 부분 수정\n",
    "    # from transformers import AdamW - 이 부분이 오류 발생\n",
    "    from torch.optim import AdamW  # 이렇게 수정\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # correct_bias 파라미터 제거\n",
    "    total_steps = len(train_data_loader) * SCHEDULER_EPOCHS#←total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0  # Early Stopping을 위한 카운터\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        # 학습\n",
    "        train_loss = train_model(model, train_data_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # 검증\n",
    "        val_loss, val_predictions, val_labels = evaluate_model(model, val_data_loader, device)\n",
    "        val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        # 최고 성능 모델 저장 및 Early Stopping 로직\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"모델 저장됨: best_model.pth (F1: {val_f1:.4f})\")\n",
    "            patience_counter = 0  # 성능이 향상되었으므로 카운터 초기화\n",
    "        else:\n",
    "            patience_counter += 1  # 성능이 향상되지 않았으므로 카운터 증가\n",
    "            print(f\"성능 향상 없음: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "            # Early Stopping 조건 확인\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly Stopping! {EARLY_STOPPING_PATIENCE}번의 에폭 동안 성능 향상 없음.\")\n",
    "                break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_predictions, test_labels = evaluate_model(model, test_data_loader, device)\n",
    "    test_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "    print(\"\\n테스트 결과:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "    # 분류 보고서 출력\n",
    "    class_names = [ID_TO_LABEL[i] for i in range(len(LABEL_DICT))]\n",
    "    print(\"\\n분류 보고서:\")\n",
    "    print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 11. 실제 테스트 데이터 예측 및 제출 파일 생성\n",
    "def predict_and_save():\n",
    "    # 테스트 파일 로드\n",
    "    test_data = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    # 텍스트 전처리 적용\n",
    "    from functools import partial\n",
    "\n",
    "    # 전처리 함수 불러오기 (위에서 사용한 함수를 가정)\n",
    "#     from preprocessing import preprocess\n",
    "\n",
    "    # 불용어 리스트 (제공된 코드에서 가져옴)\n",
    "    stopwords = '이 있 하 것 들 그 되 수 이 보 않 없 나 주 아니 등 같 우리 때 년 가 한 지 오 네 야 아 아니 그럼 내가 너'.split()\n",
    "\n",
    "    # 전처리 함수 설정 (train_preprocessed_3.csv와 동일한 설정)\n",
    "    preprocess_fn = partial(\n",
    "        preprocess,\n",
    "        stopwords=stopwords,   # 불용어 리스트  default: None\n",
    "        speaker_token=\"[UTTER]\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "        sep_token=\" \", # 줄 구분 시 토큰\n",
    "        use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "        repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    "    )\n",
    "\n",
    "    # 전처리 적용\n",
    "    test_data['processed_text'] = test_data['text'].apply(preprocess_fn)\n",
    "\n",
    "    # Dataset 생성\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_token_type_ids=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            }\n",
    "\n",
    "    # 테스트 데이터 로더 생성\n",
    "    test_dataset = TestDataset(\n",
    "        texts=test_data['processed_text'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # GPU 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # 예측\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    # 예측 결과 라벨로 변환\n",
    "    label_predictions = [str(p) for p in predictions]#←label_predictions = [ID_TO_LABEL[p] for p in predictions]\n",
    "\n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'idx': test_data['idx'],\n",
    "        'class': label_predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv('data/submission.csv', index=False)\n",
    "    print(f\"제출 파일 생성 완료: submission.csv\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 학습\n",
    "    model = train()\n",
    "\n",
    "    # 테스트 데이터 예측 및 제출 파일 생성\n",
    "    predict_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500947,
     "status": "ok",
     "timestamp": 1746465938568,
     "user": {
      "displayName": "김지환",
      "userId": "17439535786603393953"
     },
     "user_tz": -540
    },
    "id": "vYCNcsDQysRB",
    "outputId": "3e060953-aa88-4c6d-a072-abaea095e25d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리 및 로드 중...\n",
      "데이터 전처리 시작...\n",
      "   idx      class                                               text\n",
      "0    0      협박 대화  [UTTER] 지금 스스로를 죽여달라고 애원하는 것인가 [UTTER] 아닙니다 죄송...\n",
      "1    1      협박 대화  [UTTER] 길동경찰서입니다 [UTTER] 9시 40분 마트에 폭발물을 설치할거다...\n",
      "2    2  기타 괴롭힘 대화  [UTTER] 되게 귀여운거 알지 나보다 작은 남자는 첨봤어 [UTTER] 그만해 ...\n",
      "3    3      갈취 대화  [UTTER] 어이 거기 [UTTER] 예 [UTTER] 말이야 이리 오라고 [UT...\n",
      "4    4      갈취 대화  [UTTER] 저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 선크림 파는데 번 손...\n",
      "학습 데이터 크기: 3391\n",
      "검증 데이터 크기: 727\n",
      "테스트 데이터 크기: 727\n",
      "학습 데이터의 클래스 분포:\n",
      "class\n",
      "3    707\n",
      "4    700\n",
      "1    681\n",
      "2    679\n",
      "0    624\n",
      "Name: count, dtype: int64\n",
      "모델 'monologg/koelectra-base-v3-discriminator' 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda:0\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.45it/s, loss=1.2280]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8302\n",
      "Validation Macro F1: 0.7635\n",
      "모델 저장됨: best_model.pth (F1: 0.7635)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.45it/s, loss=0.6814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5302\n",
      "Validation Macro F1: 0.8419\n",
      "모델 저장됨: best_model.pth (F1: 0.8419)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.46it/s, loss=0.4438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4467\n",
      "Validation Macro F1: 0.8585\n",
      "모델 저장됨: best_model.pth (F1: 0.8585)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.45it/s, loss=0.3335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4343\n",
      "Validation Macro F1: 0.8630\n",
      "모델 저장됨: best_model.pth (F1: 0.8630)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.45it/s, loss=0.2755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4390\n",
      "Validation Macro F1: 0.8601\n",
      "성능 향상 없음: 1/2\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "학습 중: 100%|██████████| 106/106 [01:12<00:00,  1.46it/s, loss=0.2584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4390\n",
      "Validation Macro F1: 0.8601\n",
      "성능 향상 없음: 2/2\n",
      "\n",
      "Early Stopping! 2번의 에폭 동안 성능 향상 없음.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "평가 중: 100%|██████████| 23/23 [00:05<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 결과:\n",
      "Test Loss: 0.4305\n",
      "Test Macro F1: 0.8598\n",
      "\n",
      "분류 보고서:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       협박 대화       0.80      0.78      0.79       134\n",
      "       갈취 대화       0.81      0.86      0.83       146\n",
      " 직장 내 괴롭힘 대화       0.94      0.91      0.92       146\n",
      "   기타 괴롭힘 대화       0.76      0.74      0.75       151\n",
      "       일반 대화       1.00      1.00      1.00       150\n",
      "\n",
      "    accuracy                           0.86       727\n",
      "   macro avg       0.86      0.86      0.86       727\n",
      "weighted avg       0.86      0.86      0.86       727\n",
      "\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "테스트 예측 중: 100%|██████████| 16/16 [00:03<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일 생성 완료: data/submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ElectraTokenizer, ElectraForSequenceClassification#, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 시드 고정\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "#--------------------------\n",
    "# 데이터 전처리 함수들\n",
    "#--------------------------\n",
    "\n",
    "# 1. 텍스트 정제 함수들\n",
    "def remove_stopwords(tokens, stopword_list):\n",
    "    stopword_set = stopword_list if isinstance(stopword_list, set) else set(stopword_list)\n",
    "    return [token for token in tokens if token not in stopword_set]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s가-힣]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_repetitions(text, repeat_limit=2):\n",
    "    # 문자 반복 (예: ㅋㅋㅋㅋ → ㅋㅋ)\n",
    "    text = re.sub(r'(.)\\1{2,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    # 음절 반복 (예: 하하하하 → 하하)\n",
    "    text = re.sub(r'((..))\\\\1{1,}', lambda m: m.group(1) * repeat_limit, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# 2. 텍스트 정제\n",
    "def tokenize_and_clean_text(text, stopword_list=None, repeat_limit=2):\n",
    "    text = normalize_repetitions(text, repeat_limit=repeat_limit)\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "    if stopword_list:\n",
    "        tokens = remove_stopwords(tokens, stopword_list)\n",
    "    return tokens\n",
    "\n",
    "# 3. 한줄 단위 전처리\n",
    "def preprocess_conversation_lines(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    use_silence=False,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    lines = text.strip().split('\\n')\n",
    "    results = []\n",
    "\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            processed = [\"[SILENCE]\"] if use_silence else []\n",
    "        else:\n",
    "            processed = tokenize_and_clean_text(line, stopword_list=stopwords, repeat_limit=repeat_limit)\n",
    "            if not processed:\n",
    "                processed = [\"[SILENCE]\"] if use_silence else []\n",
    "            else:\n",
    "                processed = [speaker_token] + processed if speaker_token else processed\n",
    "\n",
    "        if processed:\n",
    "            results.append(\" \".join(processed).strip())\n",
    "\n",
    "    return results\n",
    "\n",
    "# 4. 여러줄을 한 줄로 flatten 함수\n",
    "def flatten_utterances(utterance_tokens_list, sep_token=\" \"):\n",
    "    return sep_token.join(utterance_tokens_list).strip()\n",
    "\n",
    "# 5. 전체 전처리 파이프라인\n",
    "def preprocess(\n",
    "    text,\n",
    "    stopwords=None,\n",
    "    speaker_token=\"[UTTER]\",\n",
    "    use_silence=True,\n",
    "    sep_token=\" \",\n",
    "    repeat_limit=2\n",
    "):\n",
    "    \"\"\"\n",
    "    전체 전처리 통합 함수\n",
    "    \"\"\"\n",
    "    utterance_tokens = preprocess_conversation_lines(\n",
    "        text,\n",
    "        stopwords=stopwords,\n",
    "        use_silence=use_silence,\n",
    "        speaker_token=speaker_token,\n",
    "        repeat_limit=repeat_limit\n",
    "    )\n",
    "    return flatten_utterances(utterance_tokens, sep_token=sep_token)\n",
    "\n",
    "# 결측치 및 중복 제거 함수\n",
    "def clean_dataframe(df, is_train=True):\n",
    "    df = df.dropna(subset=['text'])\n",
    "    df = df.drop_duplicates(subset=['text'])\n",
    "    if is_train:\n",
    "        df = df.dropna(subset=['class'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# CSV 파일 로드 함수\n",
    "def load_csv_files(file_list, is_train=True):\n",
    "    df_list = []\n",
    "\n",
    "    for file_path in file_list:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if is_train:\n",
    "            if 'conversation' in df.columns:\n",
    "                df = df.rename(columns={'conversation': 'text'})\n",
    "        else:\n",
    "            if 'conversation' in df.columns:\n",
    "                df = df.rename(columns={'conversation': 'text'})\n",
    "            if 'class' not in df.columns:\n",
    "                df['class'] = pd.NA  # test에는 class가 없으므로 NaN 처리\n",
    "\n",
    "        df_list.append(df)\n",
    "\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 데이터프레임 준비 함수\n",
    "def prepare_dataset(file_paths, preprocess_func, is_train=True):\n",
    "    df = load_csv_files(file_paths, is_train=is_train)\n",
    "    df = clean_dataframe(df, is_train=is_train)\n",
    "    df['text'] = df['text'].apply(preprocess_func)\n",
    "    df = clean_dataframe(df, is_train=is_train)\n",
    "    return df\n",
    "\n",
    "#--------------------------\n",
    "# 모델 학습 설정\n",
    "#--------------------------\n",
    "\n",
    "# 설정값\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10  # 5에서 10으로 변경\n",
    "SCHEDULER_EPOCHS = 5  # 스케줄러용 에폭 (원래 설정 유지)\n",
    "EARLY_STOPPING_PATIENCE = 2  # 연속 2번의 에폭 동안 성능이 향상되지 않으면 학습 중단\n",
    "LEARNING_RATE = 2e-5\n",
    "TRAIN_PATH = \"data/merged_train.csv\"\n",
    "TEXT_COL = \"text\"#\"conversation\"x : 편리성 때문에\n",
    "LABEL_COL = \"class\"\n",
    "LABEL_DICT = {'협박 대화': 0, '갈취 대화': 1, '직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '일반 대화': 4}\n",
    "ID_TO_LABEL = {v: k for k, v in LABEL_DICT.items()}\n",
    "GPU_NUM = 0\n",
    "TEST_FILE = \"data/test.csv\"\n",
    "\n",
    "# 1. 데이터 전처리 및 로드\n",
    "print(\"데이터 전처리 및 로드 중...\")\n",
    "\n",
    "# 불용어 리스트 정의\n",
    "stopwords = '이 있 하 것 들 그 되 수 이 보 않 없 나 주 아니 등 같 우리 때 년 가 한 지 오 네 야 아 아니 그럼 내가 너'.split()\n",
    "\n",
    "# 전처리 함수 설정 - 줄 바꿈 표시 O, 불용어 O\n",
    "preprocess_fn = partial(\n",
    "    preprocess,\n",
    "    stopwords=stopwords,   # 불용어 리스트  default: None\n",
    "    speaker_token=\"[UTTER]\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "    sep_token=\" \", # 줄 구분 시 토큰\n",
    "    use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "    repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    ")\n",
    "\n",
    "# 데이터셋 준비\n",
    "train_files = [TRAIN_PATH]\n",
    "\n",
    "# 데이터 전처리 및 저장\n",
    "print(\"데이터 전처리 시작...\")\n",
    "train_df = prepare_dataset(train_files, preprocess_fn, is_train=True)\n",
    "# train_df.to_csv(\"dataset/train_preprocessed_1.csv\", index=False)\n",
    "# print(f\"✅ 전처리된 데이터 저장 완료: {train_df.shape}\")\n",
    "print(train_df.head())\n",
    "\n",
    "# 라벨 매핑\n",
    "train_df[LABEL_COL] = train_df[LABEL_COL].map(LABEL_DICT)\n",
    "\n",
    "# 데이터 분할\n",
    "train_data, temp_data = train_test_split(train_df, test_size=0.3, random_state=42, stratify=train_df[LABEL_COL])\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data[LABEL_COL])\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_data)}\")\n",
    "print(f\"검증 데이터 크기: {len(val_data)}\")\n",
    "print(f\"테스트 데이터 크기: {len(test_data)}\")\n",
    "\n",
    "# 클래스별 데이터 개수 확인\n",
    "print(\"학습 데이터의 클래스 분포:\")\n",
    "print(train_data[LABEL_COL].value_counts())\n",
    "\n",
    "# 2. Dataset 클래스 정의\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. 데이터 로더 생성\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    dataset = TextClassificationDataset(\n",
    "        texts=df[TEXT_COL].values,\n",
    "        labels=df[LABEL_COL].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "# 4. 토크나이저 및 모델 로드\n",
    "print(f\"모델 '{MODEL_NAME}' 로드 중...\")\n",
    "tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "\n",
    "# 5. 데이터 로더 생성\n",
    "train_data_loader = create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_data, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "# 6. 모델 학습 함수\n",
    "def train_model(model, data_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    progress_bar = tqdm(data_loader, desc=\"학습 중\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\"loss\": f\"{np.mean(losses):.4f}\"})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "# 7. 모델 평가 함수\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"평가 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "            losses.append(outputs.loss.item())\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "            real_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses), predictions, real_labels\n",
    "\n",
    "# 8. 테스트 세트 예측 함수\n",
    "def predict_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 9. 혼동 행렬 시각화 함수\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    # 한글 폰트 설정\n",
    "    import matplotlib.font_manager as fm\n",
    "\n",
    "    # 시스템에 설치된 폰트 경로 확인\n",
    "    # 리눅스 환경이면 'NanumGothic'이나 다른 한글 폰트 사용\n",
    "    # 윈도우라면 'Malgun Gothic' 등 사용\n",
    "    try:\n",
    "        # 방법 1: 나눔 글꼴 설치 및 사용\n",
    "        !apt-get update -qq\n",
    "        !apt-get install fonts-nanum -qq\n",
    "        plt.rc('font', family='NanumGothic')\n",
    "    except:\n",
    "        try:\n",
    "            # 방법 2: 한글 레이블을 영어로 변환\n",
    "            classes = ['Threat', 'Extortion', 'Workplace Harassment', 'Other Harassment', 'Normal Conversation']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('예측값')\n",
    "    plt.ylabel('실제값')\n",
    "    plt.title('혼동 행렬')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "# 10. 학습 메인 함수\n",
    "def train():\n",
    "    # GPU 사용 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"사용 중인 디바이스: {device}\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 옵티마이저 및 스케줄러 설정\n",
    "    # AdamW 임포트 부분 수정\n",
    "    # from transformers import AdamW - 이 부분이 오류 발생\n",
    "    from torch.optim import AdamW  # 이렇게 수정\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # correct_bias 파라미터 제거\n",
    "    total_steps = len(train_data_loader) * SCHEDULER_EPOCHS  # 원래 에폭(5) 기준으로 설정\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0  # Early Stopping을 위한 카운터\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "        # 학습\n",
    "        train_loss = train_model(model, train_data_loader, optimizer, scheduler, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "        # 검증\n",
    "        val_loss, val_predictions, val_labels = evaluate_model(model, val_data_loader, device)\n",
    "        val_f1 = f1_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"Validation Macro F1: {val_f1:.4f}\")\n",
    "\n",
    "        # 최고 성능 모델 저장 및 Early Stopping 로직\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\"모델 저장됨: best_model.pth (F1: {val_f1:.4f})\")\n",
    "            patience_counter = 0  # 성능이 향상되었으므로 카운터 초기화\n",
    "        else:\n",
    "            patience_counter += 1  # 성능이 향상되지 않았으므로 카운터 증가\n",
    "            print(f\"성능 향상 없음: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "            # Early Stopping 조건 확인\n",
    "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"\\nEarly Stopping! {EARLY_STOPPING_PATIENCE}번의 에폭 동안 성능 향상 없음.\")\n",
    "                break\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "    # 테스트 세트 평가\n",
    "    test_loss, test_predictions, test_labels = evaluate_model(model, test_data_loader, device)\n",
    "    test_f1 = f1_score(test_labels, test_predictions, average='macro')\n",
    "\n",
    "    print(\"\\n테스트 결과:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "    # 분류 보고서 출력\n",
    "    class_names = [ID_TO_LABEL[i] for i in range(len(LABEL_DICT))]\n",
    "    print(\"\\n분류 보고서:\")\n",
    "    print(classification_report(test_labels, test_predictions, target_names=class_names))\n",
    "\n",
    "    # 혼동 행렬 시각화\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 11. 실제 테스트 데이터 예측 및 제출 파일 생성\n",
    "def predict_and_save():\n",
    "    # 테스트 파일 로드\n",
    "    test_data = pd.read_csv(TEST_FILE)\n",
    "\n",
    "    # 텍스트 전처리 적용 - 학습 데이터와 동일한 방식으로 전처리\n",
    "    # 줄 바꿈 표시 O, 불용어 O 설정\n",
    "    test_data['processed_text'] = test_data['text'].apply(\n",
    "        lambda x: preprocess(\n",
    "            x,\n",
    "            stopwords=stopwords,   # 불용어 리스트  default: None\n",
    "            speaker_token=\"[UTTER]\",  #발화 단위(줄 앞) 토큰 default: [UTTER] 사용하고 싶지 않다면 \"\"\n",
    "            sep_token=\" \", # 줄 구분 시 토큰\n",
    "            use_silence=False, #[SILENCE] 토큰을 사용할 지 여부. default: False\n",
    "            repeat_limit=2 # 반복 문자 2개까지 허용 default: 2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Dataset 생성\n",
    "    class TestDataset(Dataset):\n",
    "        def __init__(self, texts, tokenizer, max_len):\n",
    "            self.texts = texts\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            text = str(self.texts[idx])\n",
    "\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                return_token_type_ids=True,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'text': text,\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            }\n",
    "\n",
    "    # 테스트 데이터 로더 생성\n",
    "    test_dataset = TestDataset(\n",
    "        texts=test_data['processed_text'].values,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=MAX_LEN\n",
    "    )\n",
    "\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # GPU 설정\n",
    "    device = torch.device(f\"cuda:{GPU_NUM}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 최고 성능 모델 로드\n",
    "    model = ElectraForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_DICT))\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # 예측\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"테스트 예측 중\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.detach().cpu().numpy())\n",
    "\n",
    "    # 예측 결과 라벨로 변환 (숫자로 유지)\n",
    "    label_predictions = [str(p) for p in predictions]\n",
    "\n",
    "    # 제출 파일 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'idx': test_data['idx'],\n",
    "        'class': label_predictions\n",
    "    })\n",
    "\n",
    "    submission.to_csv('data/submission.csv', index=False)\n",
    "    print(f\"제출 파일 생성 완료: data/submission.csv\")\n",
    "\n",
    "# 메인 실행\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 학습\n",
    "    model = train()\n",
    "\n",
    "    # 테스트 데이터 예측 및 제출 파일 생성\n",
    "    predict_and_save()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
